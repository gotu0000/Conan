{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Circle\n",
    "import seaborn as sns; \n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parser\n",
    "import configparser\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from AISDataManager import AISDataManager\n",
    "import Constants as c\n",
    "import HMUtils as hMUtil\n",
    "import TimeUtils as timeUtils\n",
    "import GeoCompute as gC\n",
    "\n",
    "#MyConfig.INI stores all the run time constants\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../MyConfig.INI')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "aISDM = AISDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LON_MIN'])\n",
    "lonMax = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LAT_MIN'])\n",
    "latMax = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)\n",
    "\n",
    "increStep = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['INCR_STEP'])\n",
    "incrRes = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['INCR_RES'])\n",
    "\n",
    "sourceDir = config['TRAJ_PRED_LSTM_GENERAL_MORE']['SOURCE_DIR']\n",
    "trainTrajNum = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['TRAIN_DATA'])\n",
    "testEndTrajNum = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['TEST_END'])\n",
    "# dataDir = config['TRAJ_PRED_LSTM_GENERAL_MORE']['DATA_DIR']\n",
    "dataDir = \"../Data/M122_00_M119_50_34_00_36_00/General/15_16_17_3_TS_MON_DEST_TYPE_SOG_COG_GRID_TS/\"\n",
    "sogMeanFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/SOG_15_16_17_1004.npy\"\n",
    "sogVarFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/SOG_15_16_17_1004_Var.npy\"\n",
    "\n",
    "cogMeanFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/COG_15_16_17_1004_Mean.npy\"\n",
    "cogVarFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/COG_15_16_17_1004_Var.npy\"\n",
    "cogMinFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/COG_15_16_17_1004_Min.npy\"\n",
    "cogMaxFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/COG_15_16_17_1004_Max.npy\"\n",
    "cogMedianFile = \"../Data/M122_00_M119_50_34_00_36_00/Output/COG_15_16_17_1004_Median.npy\"\n",
    "print(sourceDir)\n",
    "print(dataDir)\n",
    "prevTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapGrid = hMUtil.generate_grid(lonMin, lonMax, latMin, latMax, increStep, incrRes)\n",
    "boundaryArray = heatMapGrid[2]\n",
    "horizontalAxis = heatMapGrid[0]\n",
    "verticalAxis = heatMapGrid[1]\n",
    "totalStates = horizontalAxis.shape[0] * verticalAxis.shape[0]\n",
    "print(totalStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data(sourceDir, num):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    #return LON and LAT column\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy()\n",
    "\n",
    "def get_index_from_lon_lat_cord(lon, lat):\n",
    "    retVal = -1\n",
    "#     lat = latLonRow['LAT']\n",
    "#     lon = latLonRow['LON']\n",
    "    for boundary in boundaryArray: \n",
    "        if(lon >= boundary[0]) and (lon < boundary[1]) \\\n",
    "            and (lat >= boundary[2]) and (lat < boundary[3]):\n",
    "            retVal = boundary[4]\n",
    "            break \n",
    "    return retVal\n",
    "\n",
    "def get_index_from_lon_lat(latLonRow):\n",
    "    retVal = -1\n",
    "    lat = latLonRow['LAT']\n",
    "    lon = latLonRow['LON']\n",
    "    for boundary in boundaryArray: \n",
    "        if(lon >= boundary[0]) and (lon < boundary[1]) \\\n",
    "            and (lat >= boundary[2]) and (lat < boundary[3]):\n",
    "            retVal = boundary[4]\n",
    "            break \n",
    "    return retVal\n",
    "\n",
    "def convert_traj_df_to_state_sequence(sourceDF):\n",
    "    #conver every LON and LAT to sequence of numbers\n",
    "    ret = sourceDF.apply(get_index_from_lon_lat,axis=1)\n",
    "    return ret.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sOGMean = np.load(sogMeanFile)\n",
    "sOGVar = np.load(sogVarFile)\n",
    "sOGSD = np.sqrt(sOGVar)\n",
    "sOGSD = np.nan_to_num(sOGSD)\n",
    "print(sOGMean.shape)\n",
    "print(sOGVar.shape)\n",
    "print(sOGSD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cOGMean = np.load(cogMeanFile)\n",
    "cOGVar = np.load(cogVarFile)\n",
    "cOGSD = np.sqrt(cOGVar)\n",
    "cOGSD = np.nan_to_num(cOGSD)\n",
    "cOGMedian = np.load(cogMedianFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cOGMin = np.min(cOGMean)\n",
    "cOGMax = np.max(cOGMean)\n",
    "print(cOGMin)\n",
    "print(cOGMax)\n",
    "\n",
    "cOGSDMin = np.min(cOGSD)\n",
    "cOGSDMax = np.max(cOGSD)\n",
    "print(cOGSDMin)\n",
    "print(cOGSDMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cOGMedMin = np.min(cOGMedian)\n",
    "cOGMedMax = np.max(cOGMedian)\n",
    "print(cOGMedMin)\n",
    "print(cOGMedMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data_with_len(sourceDir, num, vType = 0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe corresponding to traj number\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "#     print(sourceDF.loc[0,'DateTime'])\n",
    "    year,month,_ = sourceDF.loc[0,'DateTime'].split('-')\n",
    "    trajState = convert_traj_df_to_state_sequence(sourceDF)\n",
    "    sOGMeanNP = sOGMean[trajState].copy()\n",
    "    sOGSDNP = sOGSD[trajState].copy()\n",
    "    cOGMedianNP = cOGMedian[trajState].copy()\n",
    "    \n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy(), sourceDF.loc[0,'Length'] \\\n",
    "            , int(year), int(month), sourceDF.loc[0,'DEST_LON'], sourceDF.loc[0,'DEST_LAT'] \\\n",
    "            , vType, np.reshape(sOGMeanNP, (sOGMeanNP.shape[0],1)) \\\n",
    "            , np.reshape(sOGSDNP, (sOGSDNP.shape[0],1)) \\\n",
    "            , np.reshape(cOGMedianNP, (cOGMedianNP.shape[0],1))\n",
    "\n",
    "get_traj_lon_lat_data_with_len(sourceDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of all such trajectories\n",
    "#this is training data\n",
    "trajSeqList = []\n",
    "\n",
    "for trajNum in range(0,trainTrajNum):\n",
    "# for trajNum in range(0,2):\n",
    "    seqData = get_traj_lon_lat_data_with_len(sourceDir,trajNum)\n",
    "    trajSeqList.append(seqData)\n",
    "    if(trajNum%100)==0:\n",
    "        print(\"Done\",trajNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajSeqList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y_lon_lat(seq, lenVal, yearF, monF, destLon, destLat, vType, sOGVal, sOGSDVal, cOGVal, prevTimeStamp):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    xNumRows = seq[:-(prevTS),:].shape[0]\n",
    "        \n",
    "    monData = np.zeros((xNumRows, 12))\n",
    "    monData[:,(12-monF)] = 1\n",
    "    \n",
    "    destArr = np.zeros((xNumRows,2))\n",
    "    destArr[:,0] = destLon\n",
    "    destArr[:,1] = destLat\n",
    "    \n",
    "    typeArr = np.zeros((xNumRows,2))\n",
    "    typeArr[:,vType] = 1\n",
    "    \n",
    "    lonLatColList = []\n",
    "    sOGColList = []\n",
    "    sOGSDColList = []\n",
    "    cOGValList = []\n",
    "    for start in range(prevTimeStamp):\n",
    "        lonLatColList.append(seq[start:(-prevTimeStamp+start),:].copy())\n",
    "        sOGColList.append(sOGVal[start:(-prevTimeStamp+start),:].copy())\n",
    "        sOGSDColList.append(sOGSDVal[start:(-prevTimeStamp+start),:].copy())\n",
    "        cOGValList.append(cOGVal[start:(-prevTimeStamp+start),:].copy())\n",
    "        \n",
    "    outputLabel = seq[prevTimeStamp:,:].copy()\n",
    "    \n",
    "    xDataTS = np.zeros((xNumRows,0))\n",
    "    xDataFusion = np.zeros((xNumRows,0))\n",
    "    \n",
    "    for tS in range(prevTimeStamp):\n",
    "        xDataTS = np.hstack((xDataTS,lonLatColList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,sOGColList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,sOGSDColList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,cOGValList[tS]))\n",
    "        xDataTS = np.hstack((xDataTS,typeArr))\n",
    "        xDataTS = np.hstack((xDataTS,monData))\n",
    "        xDataTS = np.hstack((xDataTS,destArr))\n",
    "            \n",
    "#     xDataFusion = np.hstack((xDataFusion,lenData))\n",
    "#     xDataFusion = np.hstack((xDataFusion,yearData))\n",
    "#     xDataFusion = np.hstack((xDataFusion,typeArr))\n",
    "#     xDataFusion = np.hstack((xDataFusion,monData))\n",
    "#     xDataFusion = np.hstack((xDataFusion,destArr))\n",
    "    \n",
    "    return xDataTS, xDataFusion, outputLabel\n",
    "\n",
    "convert_seq_to_x_y_lon_lat(trajSeqList[0][0] \\\n",
    "                           ,trajSeqList[0][1] \\\n",
    "                           ,trajSeqList[0][2] \\\n",
    "                           ,trajSeqList[0][3] \\\n",
    "                           ,trajSeqList[0][4] \\\n",
    "                           ,trajSeqList[0][5] \\\n",
    "                           ,trajSeqList[0][6] \\\n",
    "                           ,trajSeqList[0][7] \\\n",
    "                           ,trajSeqList[0][8] \\\n",
    "                           ,trajSeqList[0][9] \\\n",
    "                           ,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate throgh trajSeqList \n",
    "#and keep on stacking them vertically\n",
    "#to make giant input and output matrix\n",
    "tSCol = 21\n",
    "numTSFeature = tSCol\n",
    "\n",
    "tSCol = tSCol * prevTS\n",
    "xDataTS = np.zeros((0,tSCol))\n",
    "\n",
    "fusionCol = 0\n",
    "xDataFusion = np.zeros((0,fusionCol))\n",
    "yData = np.zeros((0,2))\n",
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)\n",
    "for trajNum in range(len(trajSeqList)):\n",
    "    if((trajSeqList[trajNum][0].shape[0]) > prevTS):\n",
    "            \n",
    "        xTSTemp, xFusionTemp, yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum][0] \\\n",
    "            ,trajSeqList[trajNum][1] \\\n",
    "            ,trajSeqList[trajNum][2] \\\n",
    "            ,trajSeqList[trajNum][3] \\\n",
    "            ,trajSeqList[trajNum][4] \\\n",
    "            ,trajSeqList[trajNum][5] \\\n",
    "            ,trajSeqList[trajNum][6] \\\n",
    "            ,trajSeqList[trajNum][7] \\\n",
    "            ,trajSeqList[trajNum][8] \\\n",
    "            ,trajSeqList[trajNum][9] \\\n",
    "            ,prevTS)\n",
    "        xDataTS = np.vstack((xDataTS,xTSTemp.copy()))\n",
    "        xDataFusion = np.vstack((xDataFusion,xFusionTemp.copy()))\n",
    "        yData = np.vstack((yData,yTemp.copy()))\n",
    "    else:\n",
    "        print(\"Not enough trajectory\")\n",
    "    if(trajNum%100)==0:\n",
    "        print(trajNum)\n",
    "        \n",
    "# print(xDataTS)\n",
    "# print(xDataFusion)\n",
    "# print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "np.save(xTSToStore, xDataTS)\n",
    "np.save(xFToStore, xDataFusion)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "\n",
    "xDataTS = np.load(xTSToStore)\n",
    "xDataFusion = np.load(xFToStore)\n",
    "yData = np.load(yToStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sOGMin = np.min(sOGMean)\n",
    "sOGMax = np.max(sOGMean)\n",
    "print(sOGMin)\n",
    "print(sOGMax)\n",
    "\n",
    "sOGSDMin = np.min(sOGSD)\n",
    "sOGSDMax = np.max(sOGSD)\n",
    "print(sOGSDMin)\n",
    "print(sOGSDMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDataTSNorm = xDataTS.copy()\n",
    "colAccess = 0\n",
    "for prevTime in range(prevTS):\n",
    "    xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - latMin)/(latMax - latMin)\n",
    "    xDataTSNorm[:,(colAccess) + 2] = (xDataTS[:,(colAccess) + 2] - sOGMin)/(sOGMax - sOGMin)\n",
    "    xDataTSNorm[:,(colAccess) + 3] = (xDataTS[:,(colAccess) + 3] - sOGSDMin)/(sOGSDMax - sOGSDMin)\n",
    "    xDataTSNorm[:,(colAccess) + 4] = (xDataTS[:,(colAccess) + 4] - cOGMedMin)/(cOGMedMax - cOGMedMin)\n",
    "    xDataTSNorm[:,(colAccess) + 19] = (xDataTS[:,(colAccess) + 19] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 20] = (xDataTS[:,(colAccess) + 20] - latMin)/(latMax - latMin)\n",
    "    colAccess = colAccess + 21\n",
    "\n",
    "xDataFusionNorm = xDataFusion.copy()\n",
    "\n",
    "# xDataFusionNorm[:,0] = (xDataFusion[:,0] - lenMin)/(lenMax - lenMin)\n",
    "# xDataFusionNorm[:,-2] = (xDataFusion[:,-2] - lonMin)/(lonMax - lonMin)\n",
    "# xDataFusionNorm[:,-1] = (xDataFusion[:,-1] - latMin)/(latMax - latMin)\n",
    "    \n",
    "xDataTSNorm = np.reshape(xDataTSNorm,(xDataTSNorm.shape[0], prevTS, numTSFeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm.shape)\n",
    "print(xDataFusionNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - latMin)/(latMax - latMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatDataNorm = np.hstack((yLonLatData_0,yLonLatData_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm[0,:,:])\n",
    "print(xDataFusionNorm[0,:])\n",
    "print(yLonLatDataNorm[0,:])\n",
    "print(xDataTSNorm[1,:,:])\n",
    "print(xDataFusionNorm[1,:])\n",
    "print(yLonLatDataNorm[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences= True, input_shape=(2,4)))\n",
    "# model.add(LSTM(units=50))\n",
    "# model.add(Dense(150, activation='relu'))\n",
    "# model.add(Dense(units=2, activation='linear'))\n",
    "lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "hidden2 = LSTM(50)(hidden1)\n",
    "if(xDataFusionNorm.shape[1] > 0):\n",
    "    fusionIp = Input(shape=(xDataFusionNorm.shape[1],))\n",
    "    fusionIp1 = Dense(50, activation='relu')(fusionIp)\n",
    "    fusionIp2 = Dense(50, activation='relu')(fusionIp1)\n",
    "    x = concatenate([hidden2,fusionIp2])\n",
    "    lonLatDense = Dense(150, activation='relu')(x)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=[lonLatTS, fusionIp], outputs=lonLatOp)\n",
    "else:\n",
    "    lonLatDense = Dense(150, activation='relu')(hidden2)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=lonLatTS, outputs=lonLatOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xDataFusionNorm.shape[1] > 0:\n",
    "    model.fit([xDataTSNorm, xDataFusionNorm], yLonLatDataNorm, epochs=5, batch_size = 512 , verbose = 2)\n",
    "else:\n",
    "    modelHist = model.fit(xDataTSNorm, yLonLatDataNorm, epochs=1000, batch_size = 1024 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = dataDir + \"Model_1000_MSE.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lon_lat(arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "# def compute_30_min_pred(prevTraj, currTraj, typeVessel):\n",
    "def compute_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat, vType, sOGVal, sOGSDVal, cOGVal):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of previous time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "    #this will be    \n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    prevTrajNorm = normalize_lon_lat(prevTraj)\n",
    "        \n",
    "    monData = np.zeros((prevTimeStamp, 12))\n",
    "    monData[:,(12-monVal)] = 1\n",
    "    \n",
    "    destArr = np.zeros((prevTimeStamp,2))\n",
    "    destArr[:,0] = destLon\n",
    "    destArr[:,1] = destLat\n",
    "\n",
    "    destArr[:,0] = (destArr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    destArr[:,1] = (destArr[:,1] - latMin)/(latMax - latMin)\n",
    "    \n",
    "    typeData = np.zeros((prevTimeStamp,2))\n",
    "    typeData[:,vType] = 1\n",
    "    \n",
    "    xTSNorm = np.zeros((prevTimeStamp,0))\n",
    "    xFNorm = np.zeros((1,0))\n",
    "    \n",
    "    sOGValNorm = (sOGVal[:,:] - sOGMin)/(sOGMax - sOGMin)\n",
    "    sOGSDValNorm = (sOGSDVal[:,:] - sOGSDMin)/(sOGSDMax - sOGSDMin)\n",
    "    cOGValNorm = (cOGVal[:,:] - cOGMedMin)/(cOGMedMax - cOGMedMin)\n",
    "    \n",
    "    xTSNorm = np.hstack((xTSNorm,prevTrajNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,sOGValNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,sOGSDValNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,cOGValNorm))\n",
    "    xTSNorm = np.hstack((xTSNorm,typeData))\n",
    "    xTSNorm = np.hstack((xTSNorm,monData))\n",
    "    xTSNorm = np.hstack((xTSNorm,destArr))\n",
    "\n",
    "#     xFNorm = np.hstack((xFNorm,typeData))\n",
    "#     xFNorm = np.hstack((xFNorm,monData))\n",
    "#     xFNorm = np.hstack((xFNorm,destArr))\n",
    "    \n",
    "    xTSNorm = np.reshape(xTSNorm,(1,xTSNorm.shape[0],xTSNorm.shape[1]))\n",
    "    if(xFNorm.shape[1] > 0):\n",
    "        predLatLon = model.predict([xTSNorm, xFNorm])\n",
    "    else:\n",
    "        predLatLon = model.predict(xTSNorm)\n",
    "    \n",
    "    predLon = predLatLon[0,0]\n",
    "    predLat = predLatLon[0,1]\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    return predLonScaled, predLatScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat, vType, sOGVal ,sOGSDVal, cOGVal, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory\n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    \n",
    "    \n",
    "    firstLoc = prevTraj.copy()\n",
    "    firstSOG = sOGVal.copy()\n",
    "    firstSOGSD = sOGSDVal.copy()\n",
    "    firstCOG = cOGVal.copy()\n",
    "\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat = compute_30_min_pred(firstLoc, lenVal, yearVal, monVal, destLon, destLat, vType, firstSOG, firstSOGSD, firstCOG)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon,predLat]])))\n",
    "        \n",
    "        #update firstLoc \n",
    "        #for next  iteration\n",
    "        firstLoc = firstLoc[1:,:].copy()\n",
    "        firstLoc = np.vstack((firstLoc,np.array([[predLon,predLat]])))\n",
    "        firstSOG = firstSOG[1:,:].copy()\n",
    "        firstSOGSD = firstSOGSD[1:,:].copy()\n",
    "        firstCOG = firstCOG[1:,:].copy()\n",
    "        trajState = get_index_from_lon_lat_cord(predLon,predLat)\n",
    "        firstSOG = np.vstack((firstSOG,np.array([[sOGMean[trajState]]])))\n",
    "        firstSOGSD = np.vstack((firstSOGSD,np.array([[sOGSD[trajState]]])))    \n",
    "        firstCOG = np.vstack((firstCOG,np.array([[cOGMedian[trajState]]])))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(srcDir, num):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj,lenData,yearData,monData,destLon,destLat = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, n = 8)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(get_error_for_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_1004_30 = []\n",
    "trainWholeErr_1004_60 = []\n",
    "trainWholeErr_1004_90 = []\n",
    "trainWholeErr_1004_120 = []\n",
    "trainWholeErr_1004_150 = []\n",
    "trainWholeErr_1004_180 = []\n",
    "trainWholeErr_1004_210 = []\n",
    "trainWholeErr_1004_240 = []\n",
    "\n",
    "trainWholeErr_1004_n_30 = [trainWholeErr_1004_30 \\\n",
    ", trainWholeErr_1004_60 \\\n",
    ", trainWholeErr_1004_90 \\\n",
    ", trainWholeErr_1004_120 \\\n",
    ", trainWholeErr_1004_150 \\\n",
    ", trainWholeErr_1004_180 \\\n",
    ", trainWholeErr_1004_210 \\\n",
    ", trainWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_1004_30NP = np.array(trainWholeErr_1004_30)\n",
    "trainWholeErr_1004_60NP = np.array(trainWholeErr_1004_60)\n",
    "trainWholeErr_1004_90NP = np.array(trainWholeErr_1004_90)\n",
    "trainWholeErr_1004_120NP = np.array(trainWholeErr_1004_120)\n",
    "trainWholeErr_1004_150NP = np.array(trainWholeErr_1004_150)\n",
    "trainWholeErr_1004_180NP = np.array(trainWholeErr_1004_180)\n",
    "trainWholeErr_1004_210NP = np.array(trainWholeErr_1004_210)\n",
    "trainWholeErr_1004_240NP = np.array(trainWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean_1004 = [np.mean(trainWholeErr_1004_30NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_60NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_90NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_120NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_150NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_180NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_210NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "testDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum,testEndTrajNum):\n",
    "    testDataWholeErrors_1004.append(get_error_for_traj(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testWholeErr_1004_30 = []\n",
    "testWholeErr_1004_60 = []\n",
    "testWholeErr_1004_90 = []\n",
    "testWholeErr_1004_120 = []\n",
    "testWholeErr_1004_150 = []\n",
    "testWholeErr_1004_180 = []\n",
    "testWholeErr_1004_210 = []\n",
    "testWholeErr_1004_240 = []\n",
    "\n",
    "testWholeErr_1004_n_30 = [testWholeErr_1004_30 \\\n",
    ", testWholeErr_1004_60 \\\n",
    ", testWholeErr_1004_90 \\\n",
    ", testWholeErr_1004_120 \\\n",
    ", testWholeErr_1004_150 \\\n",
    ", testWholeErr_1004_180 \\\n",
    ", testWholeErr_1004_210 \\\n",
    ", testWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErr_1004_30NP = np.array(testWholeErr_1004_30)\n",
    "testWholeErr_1004_60NP = np.array(testWholeErr_1004_60)\n",
    "testWholeErr_1004_90NP = np.array(testWholeErr_1004_90)\n",
    "testWholeErr_1004_120NP = np.array(testWholeErr_1004_120)\n",
    "testWholeErr_1004_150NP = np.array(testWholeErr_1004_150)\n",
    "testWholeErr_1004_180NP = np.array(testWholeErr_1004_180)\n",
    "testWholeErr_1004_210NP = np.array(testWholeErr_1004_210)\n",
    "testWholeErr_1004_240NP = np.array(testWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErrMean_1004 = [np.mean(testWholeErr_1004_30NP) \\\n",
    "            ,np.mean(testWholeErr_1004_60NP) \\\n",
    "            ,np.mean(testWholeErr_1004_90NP) \\\n",
    "            ,np.mean(testWholeErr_1004_120NP) \\\n",
    "            ,np.mean(testWholeErr_1004_150NP) \\\n",
    "            ,np.mean(testWholeErr_1004_180NP) \\\n",
    "            ,np.mean(testWholeErr_1004_210NP) \\\n",
    "            ,np.mean(testWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(trainWholeErr_1004_30NP)\n",
    "# np.min(trainWholeErr_1004_30NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj_30(srcDir, num):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj,lenData,yearData,monData,destLon,destLat,_,sOGVal,sOGSDVal,cOGVal = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, 0, np.reshape(sOGVal[0:prevTS,:], (prevTS,1)),np.reshape(sOGSDVal[0:prevTS,:], (prevTS,1)),np.reshape(cOGVal[0:prevTS,:], (prevTS,1)),n = 1)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 1 can come from some variable too\n",
    "    #for 1 consecutive predictions\n",
    "    if(predRange > 1):\n",
    "        predRange = 1\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj_30(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxErrCount = 0;\n",
    "idx = 0\n",
    "err30Min = []\n",
    "for err in trainDataWholeErrors_1004:\n",
    "    if(len(err) > 0):\n",
    "        err30Min.append(err[0])\n",
    "        if(err[0] > 2.0):\n",
    "            print(err,idx)\n",
    "            maxErrCount = maxErrCount + 1\n",
    "    idx = idx + 1\n",
    "print(maxErrCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainTrajNum)\n",
    "print(len(trainDataWholeErrors_1004))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(err30Min)))\n",
    "print(np.median(np.array(err30Min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vesselTraj,lenData,yearData,monData,destLon,destLat,_, sOGVal,sOGSDVal = get_traj_lon_lat_data_with_len(sourceDir, 285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, 0, np.reshape(sOGVal[0:prevTS,:], (prevTS,1)),np.reshape(sOGSDVal[0:prevTS,:], (prevTS,1)), n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predVesselTraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_error_for_traj_30(sourceDir, 285)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
