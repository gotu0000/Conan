{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Circle\n",
    "import seaborn as sns; \n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parser\n",
    "import configparser\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from AISDataManager import AISDataManager\n",
    "import Constants as c\n",
    "import HMUtils as hMUtil\n",
    "import TimeUtils as timeUtils\n",
    "import GeoCompute as gC\n",
    "\n",
    "#MyConfig.INI stores all the run time constants\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../MyConfig.INI')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "aISDM = AISDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['TRAJ_PRED_LSTM_GENERAL']['LON_MIN'])\n",
    "lonMax = (float)(config['TRAJ_PRED_LSTM_GENERAL']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['TRAJ_PRED_LSTM_GENERAL']['LAT_MIN'])\n",
    "latMax = (float)(config['TRAJ_PRED_LSTM_GENERAL']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)\n",
    "\n",
    "increStep = (float)(config['TRAJ_PRED_LSTM_GENERAL']['INCR_STEP'])\n",
    "incrRes = (int)(config['TRAJ_PRED_LSTM_GENERAL']['INCR_RES'])\n",
    "\n",
    "sourceDir1 = config['TRAJ_PRED_LSTM_GENERAL']['SOURCE_DIR_1']\n",
    "sourceDir2 = config['TRAJ_PRED_LSTM_GENERAL']['SOURCE_DIR_2']\n",
    "\n",
    "trainTrajNum1 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TRAIN_DATA_1'])\n",
    "trainTrajNum2 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TRAIN_DATA_2'])\n",
    "\n",
    "testEndTrajNum1 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TEST_END_1'])\n",
    "testEndTrajNum2 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TEST_END_2'])\n",
    "\n",
    "type1 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TYPE_1'])\n",
    "type2 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['TYPE_2'])\n",
    "\n",
    "year1 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['YEAR_1'])\n",
    "year2 = (int)(config['TRAJ_PRED_LSTM_GENERAL']['YEAR_2'])\n",
    "\n",
    "dataDir = config['TRAJ_PRED_LSTM_GENERAL']['DATA_DIR']\n",
    "featuresAsTS = config['TRAJ_PRED_LSTM_GENERAL']['TS_FEATURE'].split('_')\n",
    "featuresAsFusion = config['TRAJ_PRED_LSTM_GENERAL']['FUSION_FEATURE'].split('_')\n",
    "\n",
    "prevTS = (int)(config['TRAJ_PRED_LSTM_GENERAL']['PREVIOUS_TIME_STAMP'])\n",
    "numDestination = (int)(config['TRAJ_PRED_LSTM_GENERAL']['NUM_DEST'])\n",
    "maxTypes = (int)(config['TRAJ_PRED_LSTM_GENERAL']['MAX_TYPES'])\n",
    "\n",
    "print(sourceDir1)\n",
    "print(sourceDir2)\n",
    "\n",
    "print(trainTrajNum1, testEndTrajNum1)\n",
    "print(trainTrajNum2, testEndTrajNum2)\n",
    "\n",
    "print(type1) \n",
    "print(type2)\n",
    "\n",
    "print(year1)\n",
    "print(year2)\n",
    "\n",
    "print(dataDir)\n",
    "\n",
    "print(featuresAsTS)\n",
    "print(featuresAsFusion)\n",
    "\n",
    "print(prevTS)\n",
    "print(numDestination)\n",
    "print(maxTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing block for features\n",
    "\n",
    "usefeature = { \\\n",
    "    'DONT_USE':0 \\\n",
    "    ,'USE_AS_TS':1 \\\n",
    "    ,'USE_AS_FUSION':2 \\\n",
    "}\n",
    "\n",
    "featureName = { \\\n",
    "    'Length':'Len' \\\n",
    "    ,'Type':'Type' \\\n",
    "    ,'Destination':'Dest' \\\n",
    "}\n",
    "\n",
    "#0 dont use it\n",
    "#1 use it as TS\n",
    "#2 use it as Fusion\n",
    "useLen = usefeature['DONT_USE']\n",
    "useType = usefeature['DONT_USE']\n",
    "useDest = usefeature['DONT_USE']\n",
    "\n",
    "if(featureName['Length'] in featuresAsTS):\n",
    "    useLen = usefeature['USE_AS_TS']\n",
    "elif(featureName['Length'] in featuresAsFusion):\n",
    "    useLen = usefeature['USE_AS_FUSION']\n",
    "    \n",
    "if(featureName['Type'] in featuresAsTS):\n",
    "    useType = usefeature['USE_AS_TS']\n",
    "elif(featureName['Type'] in featuresAsFusion):\n",
    "    useType = usefeature['USE_AS_FUSION']\n",
    "    \n",
    "if(featureName['Destination'] in featuresAsTS):\n",
    "    useDest = usefeature['USE_AS_TS']\n",
    "elif(featureName['Destination'] in featuresAsFusion):\n",
    "    useDest = usefeature['USE_AS_FUSION']\n",
    "    \n",
    "print(useLen)\n",
    "print(useType)\n",
    "print(useDest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselTypeSource:\n",
    "    \"\"\"\n",
    "    The VesselTypeSource object contains lots of directories related\n",
    "    information. from which we will be loading the data \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, srcDir, trainNum, testNumEnd, typeVes, year):\n",
    "        self.srcDir = srcDir\n",
    "        self.trainNum = trainNum\n",
    "        self.testNumEnd = testNumEnd\n",
    "        self.type = typeVes\n",
    "        self.year = year\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "vesselSource1 = VesselTypeSource(sourceDir1, trainTrajNum1, testEndTrajNum1, type1, year1)\n",
    "vesselSource2 = VesselTypeSource(sourceDir2, trainTrajNum2, testEndTrajNum2, type2, year2)\n",
    "\n",
    "vesselDataSources = [vesselSource1, vesselSource2]\n",
    "print(vesselDataSources[0])\n",
    "print(vesselDataSources[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data(sourceDir, num):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    #return LON and LAT column\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy()\n",
    "\n",
    "# get_traj_lon_lat_data(vesselDataSources[0].srcDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data_with_len(sourceDir, num):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe corresponding to traj number\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy(), sourceDF.loc[0,'Length']\n",
    "\n",
    "# get_traj_lon_lat_data_with_len(vesselDataSources[0].srcDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of all such trajectories\n",
    "#this is training data\n",
    "trajSeqList = []\n",
    "typeList = []\n",
    "\n",
    "if(useLen != 0):\n",
    "    lenArr = []\n",
    "    \n",
    "for vesselDataSource in vesselDataSources:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    for trajNum in range(0,vesselDataSource.trainNum):\n",
    "        if(useLen == 0):\n",
    "            seqData = get_traj_lon_lat_data(vesselDataSource.srcDir,trajNum)\n",
    "            trajSeqList.append((seqData, 0.1))\n",
    "        else:\n",
    "            seqData,lenData = get_traj_lon_lat_data_with_len(vesselDataSource.srcDir,trajNum)\n",
    "            trajSeqList.append((seqData,lenData))\n",
    "            lenArr.append(lenData)        \n",
    "            \n",
    "        typeList.append(vesselDataSource.type)\n",
    "            \n",
    "print(trajSeqList[0])\n",
    "print(typeList[0])\n",
    "    \n",
    "if(useLen != 0):\n",
    "    lenArrNP = np.array(lenArr)\n",
    "    lenMin = np.min(lenArrNP)\n",
    "    lenMax = np.max(lenArrNP)\n",
    "    print(lenMin, lenMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastPosArr = np.zeros((0,2))\n",
    "\n",
    "for vesselDataSource in vesselDataSources:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    for trajNum in range(0,vesselDataSource.testNumEnd):\n",
    "        ret = get_traj_lon_lat_data(vesselDataSource.srcDir,trajNum)\n",
    "        lastPosArr = np.vstack((lastPosArr,np.reshape(ret[-1,:],(1,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lastPosArr)\n",
    "print(lastPosArr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Kmean = KMeans(n_clusters=numDestination)\n",
    "Kmean.fit(lastPosArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Kmean.labels_)\n",
    "Kmean.cluster_centers_\n",
    "destFeature = Kmean.cluster_centers_[Kmean.labels_]\n",
    "print(destFeature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lastPosArr[:,0],lastPosArr[:,1])\n",
    "plt.scatter(Kmean.cluster_centers_[:,0],Kmean.cluster_centers_[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y_lon_lat(seq, typeVal = -1, lenVal = -1, destVal = None, prevTimeStamp = prevTS):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    xNumRows = seq[:-(prevTS),:].shape[0]\n",
    "    \n",
    "    lenData = np.zeros((xNumRows, 1))    \n",
    "    lenData[:,:] = lenVal\n",
    "    \n",
    "    typeData = np.zeros((xNumRows, maxTypes))\n",
    "    #one hot encoding\n",
    "    typeData[:,typeVal] = 1\n",
    "    \n",
    "    destArr = np.zeros((xNumRows,2))\n",
    "    destArr[:,0] = destVal[0]\n",
    "    destArr[:,1] = destVal[1]\n",
    "\n",
    "    lonLatColList = []\n",
    "    for start in range(prevTimeStamp):\n",
    "        lonLatColList.append(seq[start:(-prevTimeStamp+start),:].copy())\n",
    "        \n",
    "    outputLabel = seq[prevTimeStamp:,:].copy()\n",
    "    \n",
    "    xDataTS = np.zeros((xNumRows,0))\n",
    "    xDataFusion = np.zeros((xNumRows,0))\n",
    "    \n",
    "    for lonLatCol in lonLatColList:\n",
    "        xDataTS = np.hstack((xDataTS,lonLatCol))\n",
    "        if(useType == usefeature['USE_AS_TS']):\n",
    "            xDataTS = np.hstack((xDataTS,typeData))\n",
    "        if(useLen == usefeature['USE_AS_TS']):\n",
    "            xDataTS = np.hstack((xDataTS,lenData))\n",
    "        if(useDest == usefeature['USE_AS_TS']):\n",
    "            xDataTS = np.hstack((xDataTS,destArr))\n",
    "            \n",
    "    if(useType == usefeature['USE_AS_FUSION']):\n",
    "        xDataFusion = np.hstack((xDataFusion,typeData))\n",
    "    if(useLen == usefeature['USE_AS_FUSION']):\n",
    "        xDataFusion = np.hstack((xDataFusion,lenData))\n",
    "    if(useDest == usefeature['USE_AS_FUSION']):\n",
    "        xDataFusion = np.hstack((xDataFusion,destArr))\n",
    "    \n",
    "    return xDataTS, xDataFusion, outputLabel\n",
    "\n",
    "convert_seq_to_x_y_lon_lat(trajSeqList[0][0],typeList[0],199.94,list(destFeature[0]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate throgh trajSeqList \n",
    "#and keep on stacking them vertically\n",
    "#to make giant input and output matrix\n",
    "tSCol = 2\n",
    "if(useType == usefeature['USE_AS_TS']):\n",
    "    tSCol = tSCol + maxTypes\n",
    "if(useLen == usefeature['USE_AS_TS']):\n",
    "    tSCol = tSCol + 1\n",
    "if(useDest == usefeature['USE_AS_TS']):\n",
    "    tSCol = tSCol + 2\n",
    "    \n",
    "numTSFeature = tSCol\n",
    "\n",
    "tSCol = tSCol * prevTS\n",
    "xDataTS = np.zeros((0,tSCol))\n",
    "\n",
    "fusionCol = 0\n",
    "if(useType == usefeature['USE_AS_FUSION']):\n",
    "    fusionCol = fusionCol + maxTypes\n",
    "if(useLen == usefeature['USE_AS_FUSION']):\n",
    "    fusionCol = fusionCol + 1\n",
    "if(useDest == usefeature['USE_AS_FUSION']):\n",
    "    fusionCol = fusionCol + 2\n",
    "\n",
    "xDataFusion = np.zeros((0,fusionCol))\n",
    "yData = np.zeros((0,2))\n",
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)\n",
    "for trajNum in range(len(trajSeqList)):\n",
    "    if((trajSeqList[trajNum][0].shape[0]) > prevTS):\n",
    "        if(useLen == usefeature['DONT_USE']):\n",
    "            \n",
    "            xTSTemp, xFusionTemp, yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum][0] \\\n",
    "                                                                     ,typeVal = typeList[trajNum] \\\n",
    "                                                                     ,destVal = list(destFeature[trajNum]) \\\n",
    "                                                                     ,prevTimeStamp = prevTS)\n",
    "        else:\n",
    "            xTSTemp, xFusionTemp, yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum][0] \\\n",
    "                                                                     ,typeVal = typeList[trajNum] \\\n",
    "                                                                     ,lenVal = trajSeqList[trajNum][1] \\\n",
    "                                                                     ,destVal = list(destFeature[trajNum]) \\\n",
    "                                                                     ,prevTimeStamp = prevTS)\n",
    "        xDataTS = np.vstack((xDataTS,xTSTemp.copy()))\n",
    "        xDataFusion = np.vstack((xDataFusion,xFusionTemp.copy()))\n",
    "        yData = np.vstack((yData,yTemp.copy()))\n",
    "        \n",
    "# print(xDataTS)\n",
    "# print(xDataFusion)\n",
    "# print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "np.save(xTSToStore, xDataTS)\n",
    "np.save(xFToStore, xDataFusion)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "\n",
    "xDataTS = np.load(xTSToStore)\n",
    "xDataFusion = np.load(xFToStore)\n",
    "yData = np.load(yToStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDataTSNorm = xDataTS.copy()\n",
    "colAccess = 0\n",
    "for prevTime in range(prevTS):\n",
    "    xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - latMin)/(latMax - latMin)\n",
    "    colAccess = colAccess + 2\n",
    "    if(useType == usefeature['USE_AS_TS']):\n",
    "        colAccess = colAccess + maxTypes\n",
    "    if(useLen == usefeature['USE_AS_TS']):\n",
    "        xDataTSNorm[:,colAccess] = (xDataTS[:,colAccess] - lenMin)/(lenMax - lenMin)\n",
    "        colAccess = colAccess + 1\n",
    "    if(useDest == usefeature['USE_AS_TS']):\n",
    "        xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - lonMin)/(lonMax - lonMin)\n",
    "        xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - latMin)/(latMax - latMin)\n",
    "        colAccess = colAccess + 2\n",
    "\n",
    "colAccess = 0\n",
    "xDataFusionNorm = xDataFusion.copy()\n",
    "if(useType == usefeature['USE_AS_FUSION']):\n",
    "    colAccess = colAccess + maxTypes\n",
    "if(useLen == usefeature['USE_AS_FUSION']):\n",
    "    xDataFusionNorm[:,colAccess] = (xDataFusion[:,colAccess] - lenMin)/(lenMax - lenMin)\n",
    "    colAccess = colAccess + 1\n",
    "if(useDest == usefeature['USE_AS_FUSION']):\n",
    "    xDataFusionNorm[:,(colAccess) + 0] = (xDataFusion[:,(colAccess) + 0] - lonMin)/(lonMax - lonMin)\n",
    "    xDataFusionNorm[:,(colAccess) + 1] = (xDataFusion[:,(colAccess) + 1] - latMin)/(latMax - latMin)\n",
    "    colAccess = colAccess + 2\n",
    "    \n",
    "xDataTSNorm = np.reshape(xDataTSNorm,(xDataTSNorm.shape[0], prevTS, numTSFeature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm.shape)\n",
    "print(xDataFusionNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - latMin)/(latMax - latMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatDataNorm = np.hstack((yLonLatData_0,yLonLatData_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm[0,:])\n",
    "print(xDataFusionNorm[0,:])\n",
    "print(yLonLatDataNorm[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences= True, input_shape=(2,4)))\n",
    "# model.add(LSTM(units=50))\n",
    "# model.add(Dense(150, activation='relu'))\n",
    "# model.add(Dense(units=2, activation='linear'))\n",
    "lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "hidden2 = LSTM(50)(hidden1)\n",
    "if(xDataFusionNorm.shape[1] > 0):\n",
    "    fusionIp = Input(shape=(xDataFusionNorm.shape[1],))\n",
    "    fusionIp1 = Dense(50, activation='relu')(fusionIp)\n",
    "    fusionIp2 = Dense(50, activation='relu')(fusionIp1)\n",
    "    x = concatenate([hidden2,fusionIp2])\n",
    "    lonLatDense = Dense(150, activation='relu')(x)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=[lonLatTS, fusionIp], outputs=lonLatOp)\n",
    "else:\n",
    "    lonLatDense = Dense(150, activation='relu')(hidden2)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=lonLatTS, outputs=lonLatOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xDataFusionNorm.shape[1] > 0:\n",
    "    model.fit([xDataTSNorm, xDataFusionNorm], yLonLatDataNorm, epochs=1000, batch_size = 512 , verbose = 2)\n",
    "else:\n",
    "    \n",
    "    model.fit(xDataTSNorm, yLonLatDataNorm, epochs=1000, batch_size = 512 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = dataDir + \"Model_1000_MSE.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lon_lat(arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "# def compute_30_min_pred(prevTraj, currTraj, typeVessel):\n",
    "def compute_30_min_pred(prevTraj, typeVal, lenVal, destVal):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of previous time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "    #this will be    \n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    prevTrajNorm = normalize_lon_lat(prevTraj)\n",
    "    \n",
    "    if(useType == usefeature['USE_AS_TS']):\n",
    "        typeData = np.zeros((prevTimeStamp, maxTypes))\n",
    "        #one hot encoding\n",
    "        typeData[:,typeVal] = 1\n",
    "    elif(useType == usefeature['USE_AS_FUSION']):\n",
    "        typeData = np.zeros((1, maxTypes))\n",
    "        typeData[:,typeVal] = 1\n",
    "        \n",
    "    if(useLen == usefeature['USE_AS_TS']):\n",
    "        lenData = np.zeros((prevTimeStamp, 1))\n",
    "        lenData[:,:] = lenVal\n",
    "        lenData[:,:] = (lenData[:,:] - lenMin)/(lenMax - lenMin)\n",
    "    elif(useLen == usefeature['USE_AS_FUSION']):\n",
    "        lenData = np.zeros((1, 1))    \n",
    "        lenData[:,:] = lenVal\n",
    "        lenData[:,:] = (lenData[:,:] - lenMin)/(lenMax - lenMin)\n",
    "    \n",
    "    \n",
    "    if(useDest == usefeature['USE_AS_TS']):\n",
    "        destArr = np.zeros((prevTimeStamp,2))\n",
    "        destArr[:,0] = destVal[0]\n",
    "        destArr[:,1] = destVal[1]\n",
    "        \n",
    "        destArr[:,0] = (destArr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "        destArr[:,1] = (destArr[:,1] - latMin)/(latMax - latMin)\n",
    "    elif(useDest == usefeature['USE_AS_FUSION']):\n",
    "        destArr = np.zeros((1,2))\n",
    "        destArr[:,0] = destVal[0]\n",
    "        destArr[:,1] = destVal[1]\n",
    "        \n",
    "        destArr[:,0] = (destArr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "        destArr[:,1] = (destArr[:,1] - latMin)/(latMax - latMin)\n",
    "    \n",
    "    \n",
    "    xTSNorm = np.zeros((prevTimeStamp,0))\n",
    "    xFNorm = np.zeros((1,0))\n",
    "    \n",
    "    xTSNorm = np.hstack((xTSNorm,prevTrajNorm))\n",
    "    if(useType == usefeature['USE_AS_TS']):\n",
    "        xTSNorm = np.hstack((xTSNorm,typeData))\n",
    "    if(useLen == usefeature['USE_AS_TS']):\n",
    "        xTSNorm = np.hstack((xTSNorm,lenData))\n",
    "    if(useDest == usefeature['USE_AS_TS']):\n",
    "        xTSNorm = np.hstack((xTSNorm,destArr))\n",
    "            \n",
    "    if(useType == usefeature['USE_AS_FUSION']):\n",
    "        xFNorm = np.hstack((xFNorm,typeData))\n",
    "    if(useLen == usefeature['USE_AS_FUSION']):\n",
    "        xFNorm = np.hstack((xFNorm,lenData))\n",
    "    if(useDest == usefeature['USE_AS_FUSION']):\n",
    "        xFNorm = np.hstack((xFNorm,destArr))\n",
    "    \n",
    "    xTSNorm = np.reshape(xTSNorm,(1,xTSNorm.shape[0],xTSNorm.shape[1]))\n",
    "    if(xFNorm.shape[1] > 0):\n",
    "        predLatLon = model.predict([xTSNorm, xFNorm])\n",
    "    else:\n",
    "        predLatLon = model.predict(xTSNorm)\n",
    "    \n",
    "    predLon = predLatLon[0,0]\n",
    "    predLat = predLatLon[0,1]\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    return predLonScaled, predLatScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevTraj, typeVal, lenVal, destVal, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory\n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    \n",
    "    \n",
    "    firstLoc = prevTraj.copy()\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat = compute_30_min_pred(firstLoc, typeVal, lenVal, destVal)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon,predLat]])))\n",
    "        \n",
    "        #update firstLoc \n",
    "        #for next  iteration\n",
    "        firstLoc = firstLoc[1:,:].copy()\n",
    "        firstLoc = np.vstack((firstLoc,np.array([[predLon,predLat]])))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(srcDir, num, typeVessel, destVal):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    if(useLen == 0):\n",
    "        vesselTraj = get_traj_lon_lat_data(srcDir, num)\n",
    "        lenData = -1\n",
    "    else:\n",
    "        vesselTraj,lenData = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), typeVessel, lenData, destVal, n = 8)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(get_error_for_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(vesselDataSources[0].trainNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,vesselDataSources[0].type,destFeature[traj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_1004_30 = []\n",
    "trainWholeErr_1004_60 = []\n",
    "trainWholeErr_1004_90 = []\n",
    "trainWholeErr_1004_120 = []\n",
    "trainWholeErr_1004_150 = []\n",
    "trainWholeErr_1004_180 = []\n",
    "trainWholeErr_1004_210 = []\n",
    "trainWholeErr_1004_240 = []\n",
    "\n",
    "trainWholeErr_1004_n_30 = [trainWholeErr_1004_30 \\\n",
    ", trainWholeErr_1004_60 \\\n",
    ", trainWholeErr_1004_90 \\\n",
    ", trainWholeErr_1004_120 \\\n",
    ", trainWholeErr_1004_150 \\\n",
    ", trainWholeErr_1004_180 \\\n",
    ", trainWholeErr_1004_210 \\\n",
    ", trainWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_1004_30NP = np.array(trainWholeErr_1004_30)\n",
    "trainWholeErr_1004_60NP = np.array(trainWholeErr_1004_60)\n",
    "trainWholeErr_1004_90NP = np.array(trainWholeErr_1004_90)\n",
    "trainWholeErr_1004_120NP = np.array(trainWholeErr_1004_120)\n",
    "trainWholeErr_1004_150NP = np.array(trainWholeErr_1004_150)\n",
    "trainWholeErr_1004_180NP = np.array(trainWholeErr_1004_180)\n",
    "trainWholeErr_1004_210NP = np.array(trainWholeErr_1004_210)\n",
    "trainWholeErr_1004_240NP = np.array(trainWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean_1004 = [np.mean(trainWholeErr_1004_30NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_60NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_90NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_120NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_150NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_180NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_210NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "testDataWholeErrors_1004 = []\n",
    "for traj in range(vesselDataSources[0].trainNum,vesselDataSources[0].testNumEnd):\n",
    "    testDataWholeErrors_1004.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,vesselDataSources[0].type,destFeature[traj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testWholeErr_1004_30 = []\n",
    "testWholeErr_1004_60 = []\n",
    "testWholeErr_1004_90 = []\n",
    "testWholeErr_1004_120 = []\n",
    "testWholeErr_1004_150 = []\n",
    "testWholeErr_1004_180 = []\n",
    "testWholeErr_1004_210 = []\n",
    "testWholeErr_1004_240 = []\n",
    "\n",
    "testWholeErr_1004_n_30 = [testWholeErr_1004_30 \\\n",
    ", testWholeErr_1004_60 \\\n",
    ", testWholeErr_1004_90 \\\n",
    ", testWholeErr_1004_120 \\\n",
    ", testWholeErr_1004_150 \\\n",
    ", testWholeErr_1004_180 \\\n",
    ", testWholeErr_1004_210 \\\n",
    ", testWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErr_1004_30NP = np.array(testWholeErr_1004_30)\n",
    "testWholeErr_1004_60NP = np.array(testWholeErr_1004_60)\n",
    "testWholeErr_1004_90NP = np.array(testWholeErr_1004_90)\n",
    "testWholeErr_1004_120NP = np.array(testWholeErr_1004_120)\n",
    "testWholeErr_1004_150NP = np.array(testWholeErr_1004_150)\n",
    "testWholeErr_1004_180NP = np.array(testWholeErr_1004_180)\n",
    "testWholeErr_1004_210NP = np.array(testWholeErr_1004_210)\n",
    "testWholeErr_1004_240NP = np.array(testWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErrMean_1004 = [np.mean(testWholeErr_1004_30NP) \\\n",
    "            ,np.mean(testWholeErr_1004_60NP) \\\n",
    "            ,np.mean(testWholeErr_1004_90NP) \\\n",
    "            ,np.mean(testWholeErr_1004_120NP) \\\n",
    "            ,np.mean(testWholeErr_1004_150NP) \\\n",
    "            ,np.mean(testWholeErr_1004_180NP) \\\n",
    "            ,np.mean(testWholeErr_1004_210NP) \\\n",
    "            ,np.mean(testWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_240NP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
