{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Circle\n",
    "import seaborn as sns; \n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parser\n",
    "import configparser\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from AISDataManager import AISDataManager\n",
    "import Constants as c\n",
    "import HMUtils as hMUtil\n",
    "import TimeUtils as timeUtils\n",
    "import GeoCompute as gC\n",
    "\n",
    "#MyConfig.INI stores all the run time constants\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../MyConfig.INI')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "aISDM = AISDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['TRAJ_PRED_LSTM_3']['LON_MIN'])\n",
    "lonMax = (float)(config['TRAJ_PRED_LSTM_3']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['TRAJ_PRED_LSTM_3']['LAT_MIN'])\n",
    "latMax = (float)(config['TRAJ_PRED_LSTM_3']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)\n",
    "\n",
    "increStep = (float)(config['TRAJ_PRED_LSTM_3']['INCR_STEP'])\n",
    "incrRes = (int)(config['TRAJ_PRED_LSTM_3']['INCR_RES'])\n",
    "\n",
    "sourceDir = config['TRAJ_PRED_LSTM_3']['SOURCE_DIR']\n",
    "trainTrajNum = (int)(config['TRAJ_PRED_LSTM_3']['TRAIN_DATA'])\n",
    "testEndTrajNum = (int)(config['TRAJ_PRED_LSTM_3']['TEST_END'])\n",
    "dataDir = config['TRAJ_PRED_LSTM_3']['DATA_DIR']\n",
    "print(sourceDir)\n",
    "print(trainTrajNum)\n",
    "print(testEndTrajNum)\n",
    "print(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapGrid = hMUtil.generate_grid(lonMin, lonMax, latMin, latMax, increStep, incrRes)\n",
    "boundaryArray = heatMapGrid[2]\n",
    "horizontalAxis = heatMapGrid[0]\n",
    "verticalAxis = heatMapGrid[1]\n",
    "totalStates = horizontalAxis.shape[0] * verticalAxis.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the data frame\n",
    "#convert LON and LAT into cooresponding \n",
    "# def get_index_from_lon_lat(lat,lon):\n",
    "def get_index_from_lon_lat(latLonRow):\n",
    "    retVal = -1\n",
    "    lat = latLonRow['LAT']\n",
    "    lon = latLonRow['LON']\n",
    "    for boundary in boundaryArray: \n",
    "        if(lon >= boundary[0]) and (lon < boundary[1]) \\\n",
    "            and (lat >= boundary[2]) and (lat < boundary[3]):\n",
    "            retVal = boundary[4]\n",
    "            break \n",
    "    return retVal\n",
    "\n",
    "#will convert dataframe into sequence of numbers\n",
    "#which can be used to make data for LSTM\n",
    "def convert_traj_df_to_state_sequence(num):\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    #conver every LON and LAT to sequence of numbers\n",
    "    ret = sourceDF.apply(get_index_from_lon_lat,axis=1)\n",
    "    return ret.to_numpy()\n",
    "\n",
    "def get_traj_lon_lat_data(num):\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy()\n",
    "    \n",
    "# convert_traj_df_to_state_sequence(0)\n",
    "get_traj_lon_lat_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of all such trajectories\n",
    "#number goes from 0 to 80000\n",
    "#that much is going to be our training data\n",
    "trajSeqList = []\n",
    "\n",
    "#FIXME get number from Config file\n",
    "for trajNum in range(0,trainTrajNum):\n",
    "    trajSeqList.append(get_traj_lon_lat_data(trajNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajSeqList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes numpy array and returns x and y for\n",
    "#time series prediction\n",
    "def convert_seq_to_x_y(seq):\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    firstCol = seq[:-2].copy()\n",
    "    firstCol = np.reshape(firstCol,(firstCol.shape[0],1))\n",
    "#     print(firstCol)\n",
    "#     print(firstCol.shape)\n",
    "    #second column shifted by 1 time instances\n",
    "    secCol = seq[1:-1].copy()\n",
    "    secCol = np.reshape(secCol,(secCol.shape[0],1))\n",
    "#     print(secCol)\n",
    "#     print(secCol.shape)\n",
    "    #output is shifted by two time instances\n",
    "    outputLabel = seq[2:].copy()\n",
    "    outputLabel = np.reshape(outputLabel,(outputLabel.shape[0],1))\n",
    "#     print(outputLabel)\n",
    "#     print(outputLabel.shape)\n",
    "    xData = np.hstack((firstCol,secCol))\n",
    "#     print(xData.shape)\n",
    "    return xData, outputLabel\n",
    "\n",
    "def convert_seq_to_x_y_lon_lat(seq, prevTimeStamp = 2):\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    firstTwoCol = seq[:-3,:].copy()\n",
    "    secTwoCol = seq[1:-2].copy()\n",
    "    thirdTwoCol = seq[2:-1].copy()\n",
    "    #output is shifted by two time instances\n",
    "    outputLabel = seq[3:].copy()\n",
    "    xData = np.hstack((firstTwoCol,secTwoCol,thirdTwoCol))\n",
    "    return xData, outputLabel\n",
    "    \n",
    "#convert_seq_to_x_y_lon_lat(trajSeqList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate throgh trajSeqList \n",
    "#and keep on stacking them vertically\n",
    "#to make giant input and output matrix\n",
    "xData = np.zeros((0,6))\n",
    "yData = np.zeros((0,2))\n",
    "print(xData.shape)\n",
    "print(yData.shape)\n",
    "for trajNum in range(len(trajSeqList)):\n",
    "    if((trajSeqList[trajNum].shape[0]) > 3):\n",
    "        xTemp,yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum])\n",
    "        xData = np.vstack((xData,xTemp.copy()))\n",
    "        yData = np.vstack((yData,yTemp.copy()))\n",
    "        \n",
    "print(xData)\n",
    "print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xData.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToStore = dataDir + \"XData.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "np.save(xToStore, xData)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToStore = dataDir + \"XData.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "\n",
    "xData = np.load(xToStore)\n",
    "yData = np.load(yToStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xData.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the data\n",
    "xLonLatData_0 = (xData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "xLonLatData_0 = np.reshape(xLonLatData_0,(xLonLatData_0.shape[0],1))\n",
    "xLonLatData_1 = (xData[:,1] - latMin)/(latMax - latMin)\n",
    "xLonLatData_1 = np.reshape(xLonLatData_1,(xLonLatData_1.shape[0],1))\n",
    "xLonLatData_2 = (xData[:,2] - lonMin)/(lonMax - lonMin)\n",
    "xLonLatData_2 = np.reshape(xLonLatData_2,(xLonLatData_2.shape[0],1))\n",
    "xLonLatData_3 = (xData[:,3] - latMin)/(latMax - latMin)\n",
    "xLonLatData_3 = np.reshape(xLonLatData_3,(xLonLatData_3.shape[0],1))\n",
    "\n",
    "xLonLatData_4 = (xData[:,4] - lonMin)/(lonMax - lonMin)\n",
    "xLonLatData_4 = np.reshape(xLonLatData_4,(xLonLatData_4.shape[0],1))\n",
    "xLonLatData_5 = (xData[:,5] - latMin)/(latMax - latMin)\n",
    "xLonLatData_5 = np.reshape(xLonLatData_5,(xLonLatData_5.shape[0],1))\n",
    "\n",
    "xLonLatDataNorm = np.hstack((xLonLatData_0,xLonLatData_1,xLonLatData_2,xLonLatData_3, xLonLatData_4, xLonLatData_5))\n",
    "xLonLatDataNorm = np.reshape(xLonLatDataNorm,(xLonLatDataNorm.shape[0], 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - latMin)/(latMax - latMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatDataNorm = np.hstack((yLonLatData_0,yLonLatData_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xLonLatDataNorm[0,:,:])\n",
    "print(yLonLatDataNorm[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences= True, input_shape=(3,2)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(units=2, activation='linear'))\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=250, return_sequences= True, input_shape=(2,2)))\n",
    "# model.add(LSTM(units=250))\n",
    "# model.add(Dense(150, activation='relu'))\n",
    "# model.add(Dense(units=2, activation='linear'))\n",
    "# visible = Input(shape=(2,2))\n",
    "# hidden1 = LSTM(250, return_sequences= True)(visible)\n",
    "# hidden2 = LSTM(250)(hidden1)\n",
    "# lonLatDense = Dense(150, activation='relu')(hidden2)\n",
    "# lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "# modelFunc = Model(inputs=visible, outputs=lonLatOp)\n",
    "# modelFunc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible = Input(shape=(2,2))\n",
    "# hidden1 = LSTM(250, return_sequences= True)(visible)\n",
    "# hidden2 = LSTM(250)(hidden1)\n",
    "# lonLatDense = Dense(150, activation='relu')(hidden2)\n",
    "# lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "# modelFunc = Model(inputs=visible, outputs=lonLatOp)\n",
    "# modelFunc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xLonLatDataNorm, yLonLatDataNorm, epochs=500, batch_size = 512 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = dataDir + \"Model_500_MSE_3.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will take four arguments\n",
    "#[[lonPrev, latPrev, lonCurr, latCurr]] shape is 1x4 or nx4\n",
    "#can be vectorised also\n",
    "def normalize_lon_lat(arr):\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "def compute_30_min_pred(prevPrevTraj, prevTraj, currTraj):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of two time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "    if(prevPrevTraj.shape != (1,2)):\n",
    "        raise Exception('Shape of prevPrevTraj should be 1x2')\n",
    "        \n",
    "    if(prevTraj.shape != (1,2)):\n",
    "        raise Exception('Shape of prevTraj should be 1x2')\n",
    "    \n",
    "    if(currTraj.shape != (1,2)):\n",
    "        raise Exception('Shape of currTraj should be 1x2')\n",
    "        \n",
    "    trajX = np.vstack((prevPrevTraj,prevTraj,currTraj))\n",
    "    trajXNorm = normalize_lon_lat(trajX)\n",
    "    \n",
    "    trajXNorm = np.reshape(trajXNorm,(1,3,2))\n",
    "    predLatLon = model.predict(trajXNorm)\n",
    "    \n",
    "    predLon = predLatLon[0,0]\n",
    "    predLat = predLatLon[0,1]\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    return predLonScaled, predLatScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevPrevTraj, prevTraj, currTraj, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory \n",
    "    firstLoc = prevPrevTraj.copy()\n",
    "    secLoc = prevTraj.copy()\n",
    "    thirdLoc = currTraj.copy()\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat = compute_30_min_pred(firstLoc, secLoc, thirdLoc)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon,predLat]])))\n",
    "        \n",
    "        #update firstLoc and secLoc \n",
    "        #for next  iteration\n",
    "        \n",
    "        firstLoc = secLoc.copy()\n",
    "        secLoc = thirdLoc.copy()\n",
    "        thirdLoc = np.reshape(ret[-1,:].copy(),(1,2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(num):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    vesselTraj = get_traj_lon_lat_data(num)\n",
    "    \n",
    "    if(vesselTraj.shape[0] < 4):\n",
    "        return errorVal\n",
    "    \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0,:], (1,2)), np.reshape(vesselTraj[1,:], (1,2)), np.reshape(vesselTraj[2,:], (1,2)), n = 8)\n",
    "    predRange = vesselTraj.shape[0] - 3\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(3 + pred),0], vesselTraj[(3 + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(get_error_for_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors = []\n",
    "for traj in range(trainTrajNum):\n",
    "    trainDataWholeErrors.append(get_error_for_traj(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_30 = []\n",
    "trainWholeErr_60 = []\n",
    "trainWholeErr_90 = []\n",
    "trainWholeErr_120 = []\n",
    "trainWholeErr_150 = []\n",
    "trainWholeErr_180 = []\n",
    "trainWholeErr_210 = []\n",
    "trainWholeErr_240 = []\n",
    "\n",
    "trainWholeErr_n_30 = [trainWholeErr_30 \\\n",
    ", trainWholeErr_60 \\\n",
    ", trainWholeErr_90 \\\n",
    ", trainWholeErr_120 \\\n",
    ", trainWholeErr_150 \\\n",
    ", trainWholeErr_180 \\\n",
    ", trainWholeErr_210 \\\n",
    ", trainWholeErr_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_30NP = np.array(trainWholeErr_30)\n",
    "trainWholeErr_60NP = np.array(trainWholeErr_60)\n",
    "trainWholeErr_90NP = np.array(trainWholeErr_90)\n",
    "trainWholeErr_120NP = np.array(trainWholeErr_120)\n",
    "trainWholeErr_150NP = np.array(trainWholeErr_150)\n",
    "trainWholeErr_180NP = np.array(trainWholeErr_180)\n",
    "trainWholeErr_210NP = np.array(trainWholeErr_210)\n",
    "trainWholeErr_240NP = np.array(trainWholeErr_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean = [np.mean(trainWholeErr_30NP) \\\n",
    "            ,np.mean(trainWholeErr_60NP) \\\n",
    "            ,np.mean(trainWholeErr_90NP) \\\n",
    "            ,np.mean(trainWholeErr_120NP) \\\n",
    "            ,np.mean(trainWholeErr_150NP) \\\n",
    "            ,np.mean(trainWholeErr_180NP) \\\n",
    "            ,np.mean(trainWholeErr_210NP) \\\n",
    "            ,np.mean(trainWholeErr_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all testing trajectories\n",
    "testDataWholeErrors = []\n",
    "for traj in range(trainTrajNum, testEndTrajNum):\n",
    "    testDataWholeErrors.append(get_error_for_traj(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testWholeErr_30 = []\n",
    "testWholeErr_60 = []\n",
    "testWholeErr_90 = []\n",
    "testWholeErr_120 = []\n",
    "testWholeErr_150 = []\n",
    "testWholeErr_180 = []\n",
    "testWholeErr_210 = []\n",
    "testWholeErr_240 = []\n",
    "\n",
    "testWholeErr_n_30 = [testWholeErr_30 \\\n",
    ", testWholeErr_60 \\\n",
    ", testWholeErr_90 \\\n",
    ", testWholeErr_120 \\\n",
    ", testWholeErr_150 \\\n",
    ", testWholeErr_180 \\\n",
    ", testWholeErr_210 \\\n",
    ", testWholeErr_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErr_30NP = np.array(testWholeErr_30)\n",
    "testWholeErr_60NP = np.array(testWholeErr_60)\n",
    "testWholeErr_90NP = np.array(testWholeErr_90)\n",
    "testWholeErr_120NP = np.array(testWholeErr_120)\n",
    "testWholeErr_150NP = np.array(testWholeErr_150)\n",
    "testWholeErr_180NP = np.array(testWholeErr_180)\n",
    "testWholeErr_210NP = np.array(testWholeErr_210)\n",
    "testWholeErr_240NP = np.array(testWholeErr_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErrMean = [np.mean(testWholeErr_30NP) \\\n",
    "            ,np.mean(testWholeErr_60NP) \\\n",
    "            ,np.mean(testWholeErr_90NP) \\\n",
    "            ,np.mean(testWholeErr_120NP) \\\n",
    "            ,np.mean(testWholeErr_150NP) \\\n",
    "            ,np.mean(testWholeErr_180NP) \\\n",
    "            ,np.mean(testWholeErr_210NP) \\\n",
    "            ,np.mean(testWholeErr_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand1000TrajTrain = np.random.randint(0,trainTrajNum,size = 1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainR1000Err = []\n",
    "for traj in rand1000TrajTrain:\n",
    "    trainR1000Err.append(get_error_for_traj(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainR1000Err_30 = []\n",
    "trainR1000Err_60 = []\n",
    "trainR1000Err_90 = []\n",
    "trainR1000Err_120 = []\n",
    "trainR1000Err_150 = []\n",
    "trainR1000Err_180 = []\n",
    "trainR1000Err_210 = []\n",
    "trainR1000Err_240 = []\n",
    "\n",
    "trainR1000Err_n_30 = [trainR1000Err_30 \\\n",
    ", trainR1000Err_60 \\\n",
    ", trainR1000Err_90 \\\n",
    ", trainR1000Err_120 \\\n",
    ", trainR1000Err_150 \\\n",
    ", trainR1000Err_180 \\\n",
    ", trainR1000Err_210 \\\n",
    ", trainR1000Err_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainR1000Err:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainR1000Err_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainR1000Err_30NP = np.array(trainR1000Err_30)\n",
    "trainR1000Err_60NP = np.array(trainR1000Err_60)\n",
    "trainR1000Err_90NP = np.array(trainR1000Err_90)\n",
    "trainR1000Err_120NP = np.array(trainR1000Err_120)\n",
    "trainR1000Err_150NP = np.array(trainR1000Err_150)\n",
    "trainR1000Err_180NP = np.array(trainR1000Err_180)\n",
    "trainR1000Err_210NP = np.array(trainR1000Err_210)\n",
    "trainR1000Err_240NP = np.array(trainR1000Err_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainR1000ErrMean = [np.mean(trainR1000Err_30NP) \\\n",
    "            ,np.mean(trainR1000Err_60NP) \\\n",
    "            ,np.mean(trainR1000Err_90NP) \\\n",
    "            ,np.mean(trainR1000Err_120NP) \\\n",
    "            ,np.mean(trainR1000Err_150NP) \\\n",
    "            ,np.mean(trainR1000Err_180NP) \\\n",
    "            ,np.mean(trainR1000Err_210NP) \\\n",
    "            ,np.mean(trainR1000Err_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error for 1000 Training Trajectories\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainR1000ErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "rand1000TrajTest = np.random.randint(trainTrajNum,testEndTrajNum,size = 1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all testing trajectories\n",
    "testR1000Err = []\n",
    "for traj in rand1000TrajTest:\n",
    "    testR1000Err.append(get_error_for_traj(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testR1000Err_30 = []\n",
    "testR1000Err_60 = []\n",
    "testR1000Err_90 = []\n",
    "testR1000Err_120 = []\n",
    "testR1000Err_150 = []\n",
    "testR1000Err_180 = []\n",
    "testR1000Err_210 = []\n",
    "testR1000Err_240 = []\n",
    "\n",
    "testR1000Err_n_30 = [testR1000Err_30 \\\n",
    ", testR1000Err_60 \\\n",
    ", testR1000Err_90 \\\n",
    ", testR1000Err_120 \\\n",
    ", testR1000Err_150 \\\n",
    ", testR1000Err_180 \\\n",
    ", testR1000Err_210 \\\n",
    ", testR1000Err_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testR1000Err:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testR1000Err_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR1000Err_30NP = np.array(testR1000Err_30)\n",
    "testR1000Err_60NP = np.array(testR1000Err_60)\n",
    "testR1000Err_90NP = np.array(testR1000Err_90)\n",
    "testR1000Err_120NP = np.array(testR1000Err_120)\n",
    "testR1000Err_150NP = np.array(testR1000Err_150)\n",
    "testR1000Err_180NP = np.array(testR1000Err_180)\n",
    "testR1000Err_210NP = np.array(testR1000Err_210)\n",
    "testR1000Err_240NP = np.array(testR1000Err_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR1000ErrMean = [np.mean(testR1000Err_30NP) \\\n",
    "            ,np.mean(testR1000Err_60NP) \\\n",
    "            ,np.mean(testR1000Err_90NP) \\\n",
    "            ,np.mean(testR1000Err_120NP) \\\n",
    "            ,np.mean(testR1000Err_150NP) \\\n",
    "            ,np.mean(testR1000Err_180NP) \\\n",
    "            ,np.mean(testR1000Err_210NP) \\\n",
    "            ,np.mean(testR1000Err_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error for 1000 Testing Trajectories\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testR1000ErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainWholeErr_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_30NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_60NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_90NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_120NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_150NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_180NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_210NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testWholeErr_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_30NP)\n",
    "dataToStore = dataDir + \"testWholeErr_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_60NP)\n",
    "dataToStore = dataDir + \"testWholeErr_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_90NP)\n",
    "dataToStore = dataDir + \"testWholeErr_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_120NP)\n",
    "dataToStore = dataDir + \"testWholeErr_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_150NP)\n",
    "dataToStore = dataDir + \"testWholeErr_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_180NP)\n",
    "dataToStore = dataDir + \"testWholeErr_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_210NP)\n",
    "dataToStore = dataDir + \"testWholeErr_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainR1000Err_30NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_30NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_60NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_60NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_90NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_90NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_120NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_120NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_150NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_150NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_180NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_180NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_210NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_210NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_240NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testR1000Err_30NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_30NP)\n",
    "dataToStore = dataDir + \"testR1000Err_60NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_60NP)\n",
    "dataToStore = dataDir + \"testR1000Err_90NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_90NP)\n",
    "dataToStore = dataDir + \"testR1000Err_120NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_120NP)\n",
    "dataToStore = dataDir + \"testR1000Err_150NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_150NP)\n",
    "dataToStore = dataDir + \"testR1000Err_180NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_180NP)\n",
    "dataToStore = dataDir + \"testR1000Err_210NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_210NP)\n",
    "dataToStore = dataDir + \"testR1000Err_240NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_240NP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
