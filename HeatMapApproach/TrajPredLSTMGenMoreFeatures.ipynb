{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Circle\n",
    "import seaborn as sns; \n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parser\n",
    "import configparser\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from AISDataManager import AISDataManager\n",
    "import Constants as c\n",
    "import HMUtils as hMUtil\n",
    "import TimeUtils as timeUtils\n",
    "import GeoCompute as gC\n",
    "\n",
    "#MyConfig.INI stores all the run time constants\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../MyConfig.INI')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "aISDM = AISDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LON_MIN'])\n",
    "lonMax = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LAT_MIN'])\n",
    "latMax = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)\n",
    "\n",
    "increStep = (float)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['INCR_STEP'])\n",
    "incrRes = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['INCR_RES'])\n",
    "\n",
    "sourceDir = config['TRAJ_PRED_LSTM_GENERAL_MORE']['SOURCE_DIR']\n",
    "trainTrajNum = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['TRAIN_DATA'])\n",
    "testEndTrajNum = (int)(config['TRAJ_PRED_LSTM_GENERAL_MORE']['TEST_END'])\n",
    "dataDir = config['TRAJ_PRED_LSTM_GENERAL_MORE']['DATA_DIR']\n",
    "print(sourceDir)\n",
    "print(dataDir)\n",
    "prevTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data(sourceDir, num):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    #return LON and LAT column\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy()\n",
    "\n",
    "# get_traj_lon_lat_data(vesselDataSources[0].srcDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data_with_len(sourceDir, num):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #read the dataframe corresponding to traj number\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    print(sourceDF.loc[0,'DateTime'])\n",
    "    year,month,_ = sourceDF.loc[0,'DateTime'].split('-')\n",
    "#     print(sourceDF.loc[0,'DEST_LON'])\n",
    "#     print(sourceDF.loc[0,'DEST_LAT'])\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy(), sourceDF.loc[0,'Length'] \\\n",
    "            , int(year), int(month), sourceDF.loc[0,'DEST_LON'], sourceDF.loc[0,'DEST_LAT']\n",
    "\n",
    "get_traj_lon_lat_data_with_len(sourceDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of all such trajectories\n",
    "#this is training data\n",
    "trajSeqList = []\n",
    "\n",
    "for trajNum in range(0,trainTrajNum):\n",
    "    seqData = get_traj_lon_lat_data_with_len(sourceDir,trajNum)\n",
    "    trajSeqList.append(seqData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajSeqList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y_lon_lat(seq, lenVal, yearF, monF, destLon, destLat, prevTimeStamp = prevTS):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    xNumRows = seq[:-(prevTS),:].shape[0]\n",
    "    \n",
    "    lenData = np.zeros((xNumRows, 1))\n",
    "    lenData[:,:] = lenVal\n",
    "    \n",
    "    yearData = np.zeros((xNumRows, 2))\n",
    "    yearData[:,(2017-yearF)] = 1\n",
    "    monData = np.zeros((xNumRows, 12))\n",
    "    monData[:,(12-monF)] = 1\n",
    "    \n",
    "    destArr = np.zeros((xNumRows,2))\n",
    "    destArr[:,0] = destLon\n",
    "    destArr[:,1] = destLat\n",
    "\n",
    "    lonLatColList = []\n",
    "    for start in range(prevTimeStamp):\n",
    "        lonLatColList.append(seq[start:(-prevTimeStamp+start),:].copy())\n",
    "        \n",
    "    outputLabel = seq[prevTimeStamp:,:].copy()\n",
    "    \n",
    "    xDataTS = np.zeros((xNumRows,0))\n",
    "    xDataFusion = np.zeros((xNumRows,0))\n",
    "    \n",
    "    for lonLatCol in lonLatColList:\n",
    "        xDataTS = np.hstack((xDataTS,lonLatCol))\n",
    "#         xDataTS = np.hstack((xDataTS,typeData))\n",
    "#         xDataTS = np.hstack((xDataTS,lenData))\n",
    "#         xDataTS = np.hstack((xDataTS,destArr))\n",
    "            \n",
    "    xDataFusion = np.hstack((xDataFusion,lenData))\n",
    "    xDataFusion = np.hstack((xDataFusion,yearData))\n",
    "    xDataFusion = np.hstack((xDataFusion,monData))\n",
    "    xDataFusion = np.hstack((xDataFusion,destArr))\n",
    "    \n",
    "    return xDataTS, xDataFusion, outputLabel\n",
    "\n",
    "convert_seq_to_x_y_lon_lat(trajSeqList[0][0] \\\n",
    "                           ,trajSeqList[0][1] \\\n",
    "                           ,trajSeqList[0][2] \\\n",
    "                           ,trajSeqList[0][3] \\\n",
    "                           ,trajSeqList[0][4] \\\n",
    "                           ,trajSeqList[0][5] \\\n",
    "                           ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate throgh trajSeqList \n",
    "#and keep on stacking them vertically\n",
    "#to make giant input and output matrix\n",
    "tSCol = 2\n",
    "numTSFeature = tSCol\n",
    "\n",
    "tSCol = tSCol * prevTS\n",
    "xDataTS = np.zeros((0,tSCol))\n",
    "\n",
    "fusionCol = 17\n",
    "xDataFusion = np.zeros((0,fusionCol))\n",
    "yData = np.zeros((0,2))\n",
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)\n",
    "for trajNum in range(len(trajSeqList)):\n",
    "    if((trajSeqList[trajNum][0].shape[0]) > prevTS):\n",
    "            \n",
    "        xTSTemp, xFusionTemp, yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum][0] \\\n",
    "            ,trajSeqList[trajNum][1] \\\n",
    "            ,trajSeqList[trajNum][2] \\\n",
    "            ,trajSeqList[trajNum][3] \\\n",
    "            ,trajSeqList[trajNum][4] \\\n",
    "            ,trajSeqList[trajNum][5] \\\n",
    "            ,prevTS)\n",
    "        xDataTS = np.vstack((xDataTS,xTSTemp.copy()))\n",
    "        xDataFusion = np.vstack((xDataFusion,xFusionTemp.copy()))\n",
    "        yData = np.vstack((yData,yTemp.copy()))\n",
    "        \n",
    "# print(xDataTS)\n",
    "# print(xDataFusion)\n",
    "# print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "np.save(xTSToStore, xDataTS)\n",
    "np.save(xFToStore, xDataFusion)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTSToStore = dataDir + \"XDataTS.npy\"\n",
    "xFToStore = dataDir + \"XDataF.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "\n",
    "xDataTS = np.load(xTSToStore)\n",
    "xDataFusion = np.load(xFToStore)\n",
    "yData = np.load(yToStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTS.shape)\n",
    "print(xDataFusion.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenMin = np.min(xDataFusion[:,0])\n",
    "lenMax = np.max(xDataFusion[:,0])\n",
    "print(lenMin, lenMax)\n",
    "print(np.argmax(xDataFusion[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDataTSNorm = xDataTS.copy()\n",
    "colAccess = 0\n",
    "for prevTime in range(prevTS):\n",
    "    xDataTSNorm[:,(colAccess) + 0] = (xDataTS[:,(colAccess) + 0] - lonMin)/(lonMax - lonMin)\n",
    "    xDataTSNorm[:,(colAccess) + 1] = (xDataTS[:,(colAccess) + 1] - latMin)/(latMax - latMin)\n",
    "    colAccess = colAccess + 2\n",
    "\n",
    "xDataFusionNorm = xDataFusion.copy()\n",
    "\n",
    "xDataFusionNorm[:,0] = (xDataFusion[:,0] - lenMin)/(lenMax - lenMin)\n",
    "xDataFusionNorm[:,-2] = (xDataFusion[:,-2] - lonMin)/(lonMax - lonMin)\n",
    "xDataFusionNorm[:,-1] = (xDataFusion[:,-1] - latMin)/(latMax - latMin)\n",
    "    \n",
    "xDataTSNorm = np.reshape(xDataTSNorm,(xDataTSNorm.shape[0], prevTS, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm.shape)\n",
    "print(xDataFusionNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - latMin)/(latMax - latMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatDataNorm = np.hstack((yLonLatData_0,yLonLatData_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xDataTSNorm[0,:])\n",
    "print(xDataFusionNorm[0,:])\n",
    "print(yLonLatDataNorm[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences= True, input_shape=(2,4)))\n",
    "# model.add(LSTM(units=50))\n",
    "# model.add(Dense(150, activation='relu'))\n",
    "# model.add(Dense(units=2, activation='linear'))\n",
    "lonLatTS = Input(shape=(prevTS,numTSFeature))\n",
    "hidden1 = LSTM(50, return_sequences= True)(lonLatTS)\n",
    "hidden2 = LSTM(50)(hidden1)\n",
    "if(xDataFusionNorm.shape[1] > 0):\n",
    "    fusionIp = Input(shape=(xDataFusionNorm.shape[1],))\n",
    "    fusionIp1 = Dense(50, activation='relu')(fusionIp)\n",
    "    fusionIp2 = Dense(50, activation='relu')(fusionIp1)\n",
    "    x = concatenate([hidden2,fusionIp2])\n",
    "    lonLatDense = Dense(150, activation='relu')(x)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=[lonLatTS, fusionIp], outputs=lonLatOp)\n",
    "else:\n",
    "    lonLatDense = Dense(150, activation='relu')(hidden2)\n",
    "    lonLatOp = Dense(2, activation='linear')(lonLatDense)\n",
    "    model = Model(inputs=lonLatTS, outputs=lonLatOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xDataFusionNorm.shape[1] > 0:\n",
    "    model.fit([xDataTSNorm, xDataFusionNorm], yLonLatDataNorm, epochs=1000, batch_size = 512 , verbose = 2)\n",
    "else:\n",
    "    \n",
    "    model.fit(xDataTSNorm, yLonLatDataNorm, epochs=1000, batch_size = 512 , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = dataDir + \"Model_1000_MSE.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lon_lat(arr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "# def compute_30_min_pred(prevTraj, currTraj, typeVessel):\n",
    "def compute_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of previous time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "    #this will be    \n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    prevTrajNorm = normalize_lon_lat(prevTraj)\n",
    "        \n",
    "    lenData = np.zeros((1, 1))    \n",
    "    lenData[:,:] = lenVal\n",
    "    lenData[:,:] = (lenData[:,:] - lenMin)/(lenMax - lenMin)\n",
    "    \n",
    "    yearData = np.zeros((1, 2))\n",
    "    yearData[:,(2017-yearVal)] = 1\n",
    "    monData = np.zeros((1, 12))\n",
    "    monData[:,(12-monVal)] = 1\n",
    "    \n",
    "    destArr = np.zeros((1,2))\n",
    "    destArr[:,0] = destLon\n",
    "    destArr[:,1] = destLat\n",
    "\n",
    "    destArr[:,0] = (destArr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    destArr[:,1] = (destArr[:,1] - latMin)/(latMax - latMin)  \n",
    "    \n",
    "    xTSNorm = np.zeros((prevTimeStamp,0))\n",
    "    xFNorm = np.zeros((1,0))\n",
    "    \n",
    "    xTSNorm = np.hstack((xTSNorm,prevTrajNorm))\n",
    "            \n",
    "    xFNorm = np.hstack((xFNorm,lenData))\n",
    "    xFNorm = np.hstack((xFNorm,yearData))\n",
    "    xFNorm = np.hstack((xFNorm,monData))\n",
    "    xFNorm = np.hstack((xFNorm,destArr))\n",
    "    \n",
    "    xTSNorm = np.reshape(xTSNorm,(1,xTSNorm.shape[0],xTSNorm.shape[1]))\n",
    "    if(xFNorm.shape[1] > 0):\n",
    "        predLatLon = model.predict([xTSNorm, xFNorm])\n",
    "    else:\n",
    "        predLatLon = model.predict(xTSNorm)\n",
    "    \n",
    "    predLon = predLatLon[0,0]\n",
    "    predLat = predLatLon[0,1]\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    return predLonScaled, predLatScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevTraj, lenVal, yearVal, monVal, destLon, destLat, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory\n",
    "    prevTimeStamp = prevTraj.shape[0]\n",
    "    \n",
    "    \n",
    "    firstLoc = prevTraj.copy()\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat = compute_30_min_pred(firstLoc, lenVal, yearVal, monVal, destLon, destLat)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon,predLat]])))\n",
    "        \n",
    "        #update firstLoc \n",
    "        #for next  iteration\n",
    "        firstLoc = firstLoc[1:,:].copy()\n",
    "        firstLoc = np.vstack((firstLoc,np.array([[predLon,predLat]])))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(srcDir, num):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj,lenData,yearData,monData,destLon,destLat = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, n = 8)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(get_error_for_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_1004_30 = []\n",
    "trainWholeErr_1004_60 = []\n",
    "trainWholeErr_1004_90 = []\n",
    "trainWholeErr_1004_120 = []\n",
    "trainWholeErr_1004_150 = []\n",
    "trainWholeErr_1004_180 = []\n",
    "trainWholeErr_1004_210 = []\n",
    "trainWholeErr_1004_240 = []\n",
    "\n",
    "trainWholeErr_1004_n_30 = [trainWholeErr_1004_30 \\\n",
    ", trainWholeErr_1004_60 \\\n",
    ", trainWholeErr_1004_90 \\\n",
    ", trainWholeErr_1004_120 \\\n",
    ", trainWholeErr_1004_150 \\\n",
    ", trainWholeErr_1004_180 \\\n",
    ", trainWholeErr_1004_210 \\\n",
    ", trainWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_1004_30NP = np.array(trainWholeErr_1004_30)\n",
    "trainWholeErr_1004_60NP = np.array(trainWholeErr_1004_60)\n",
    "trainWholeErr_1004_90NP = np.array(trainWholeErr_1004_90)\n",
    "trainWholeErr_1004_120NP = np.array(trainWholeErr_1004_120)\n",
    "trainWholeErr_1004_150NP = np.array(trainWholeErr_1004_150)\n",
    "trainWholeErr_1004_180NP = np.array(trainWholeErr_1004_180)\n",
    "trainWholeErr_1004_210NP = np.array(trainWholeErr_1004_210)\n",
    "trainWholeErr_1004_240NP = np.array(trainWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean_1004 = [np.mean(trainWholeErr_1004_30NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_60NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_90NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_120NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_150NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_180NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_210NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "testDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum,testEndTrajNum):\n",
    "    testDataWholeErrors_1004.append(get_error_for_traj(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testWholeErr_1004_30 = []\n",
    "testWholeErr_1004_60 = []\n",
    "testWholeErr_1004_90 = []\n",
    "testWholeErr_1004_120 = []\n",
    "testWholeErr_1004_150 = []\n",
    "testWholeErr_1004_180 = []\n",
    "testWholeErr_1004_210 = []\n",
    "testWholeErr_1004_240 = []\n",
    "\n",
    "testWholeErr_1004_n_30 = [testWholeErr_1004_30 \\\n",
    ", testWholeErr_1004_60 \\\n",
    ", testWholeErr_1004_90 \\\n",
    ", testWholeErr_1004_120 \\\n",
    ", testWholeErr_1004_150 \\\n",
    ", testWholeErr_1004_180 \\\n",
    ", testWholeErr_1004_210 \\\n",
    ", testWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErr_1004_30NP = np.array(testWholeErr_1004_30)\n",
    "testWholeErr_1004_60NP = np.array(testWholeErr_1004_60)\n",
    "testWholeErr_1004_90NP = np.array(testWholeErr_1004_90)\n",
    "testWholeErr_1004_120NP = np.array(testWholeErr_1004_120)\n",
    "testWholeErr_1004_150NP = np.array(testWholeErr_1004_150)\n",
    "testWholeErr_1004_180NP = np.array(testWholeErr_1004_180)\n",
    "testWholeErr_1004_210NP = np.array(testWholeErr_1004_210)\n",
    "testWholeErr_1004_240NP = np.array(testWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErrMean_1004 = [np.mean(testWholeErr_1004_30NP) \\\n",
    "            ,np.mean(testWholeErr_1004_60NP) \\\n",
    "            ,np.mean(testWholeErr_1004_90NP) \\\n",
    "            ,np.mean(testWholeErr_1004_120NP) \\\n",
    "            ,np.mean(testWholeErr_1004_150NP) \\\n",
    "            ,np.mean(testWholeErr_1004_180NP) \\\n",
    "            ,np.mean(testWholeErr_1004_210NP) \\\n",
    "            ,np.mean(testWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(trainWholeErr_1004_30NP)\n",
    "# np.min(trainWholeErr_1004_30NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj_30(srcDir, num):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    \n",
    "    vesselTraj,lenData,yearData,monData,destLon,destLat = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < (prevTS + 1)):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0:prevTS,:], (prevTS,2)), lenData, yearData, monData, destLon, destLat, n = 1)\n",
    "    predRange = vesselTraj.shape[0] - prevTS\n",
    "\n",
    "    #FIXME 1 can come from some variable too\n",
    "    #for 1 consecutive predictions\n",
    "    if(predRange > 1):\n",
    "        predRange = 1\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(prevTS + pred),0], vesselTraj[(prevTS + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(trainTrajNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj_30(sourceDir,traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxErrCount = 0;\n",
    "idx = 0\n",
    "for err in trainDataWholeErrors_1004:\n",
    "    idx = idx + 1\n",
    "    if(err[0] > 2):\n",
    "        print(err,idx)\n",
    "        maxErrCount = maxErrCount + 1\n",
    "print(maxErrCount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
