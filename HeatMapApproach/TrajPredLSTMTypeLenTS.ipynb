{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Circle\n",
    "import seaborn as sns; \n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config parser\n",
    "import configparser\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from AISDataManager import AISDataManager\n",
    "import Constants as c\n",
    "import HMUtils as hMUtil\n",
    "import TimeUtils as timeUtils\n",
    "import GeoCompute as gC\n",
    "\n",
    "#MyConfig.INI stores all the run time constants\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../MyConfig.INI')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "aISDM = AISDataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonMin = (float)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['LON_MIN'])\n",
    "lonMax = (float)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['LON_MAX'])\n",
    "\n",
    "latMin = (float)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['LAT_MIN'])\n",
    "latMax = (float)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['LAT_MAX'])\n",
    "\n",
    "print(lonMin,latMin)\n",
    "print(lonMax,latMax)\n",
    "\n",
    "increStep = (float)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['INCR_STEP'])\n",
    "incrRes = (int)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['INCR_RES'])\n",
    "\n",
    "sourceDir1 = config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['SOURCE_DIR_1']\n",
    "sourceDir2 = config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['SOURCE_DIR_2']\n",
    "\n",
    "trainTrajNum1 = (int)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['TRAIN_DATA_1'])\n",
    "trainTrajNum2 = (int)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['TRAIN_DATA_2'])\n",
    "\n",
    "testEndTrajNum1 = (int)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['TEST_END_1'])\n",
    "testEndTrajNum2 = (int)(config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['TEST_END_2'])\n",
    "\n",
    "dataDir = config['TRAJ_PRED_LSTM_TYPE_LEN_TS']['DATA_DIR']\n",
    "    \n",
    "print(sourceDir1)\n",
    "print(sourceDir2)\n",
    "print(trainTrajNum1, testEndTrajNum1)\n",
    "print(trainTrajNum2, testEndTrajNum2)\n",
    "\n",
    "print(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselTypeSource:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, srcDir, trainNum, testNumEnd):\n",
    "        self.srcDir = srcDir\n",
    "        self.trainNum = trainNum\n",
    "        self.testNumEnd = testNumEnd\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "cargo1004 = VesselTypeSource(sourceDir1, trainTrajNum1, testEndTrajNum1)\n",
    "tanker1024 = VesselTypeSource(sourceDir2, trainTrajNum2, testEndTrajNum2)\n",
    "\n",
    "vesselDataSources = [cargo1004, tanker1024]\n",
    "print(vesselDataSources[0])\n",
    "print(vesselDataSources[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapGrid = hMUtil.generate_grid(lonMin, lonMax, latMin, latMax, increStep, incrRes)\n",
    "boundaryArray = heatMapGrid[2]\n",
    "horizontalAxis = heatMapGrid[0]\n",
    "verticalAxis = heatMapGrid[1]\n",
    "totalStates = horizontalAxis.shape[0] * verticalAxis.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data(sourceDir, num):\n",
    "    #read the dataframe\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy()\n",
    "    \n",
    "get_traj_lon_lat_data(vesselDataSources[0].srcDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_lon_lat_data_with_len(sourceDir, num):\n",
    "    sorceFile = sourceDir + str(num) + '.csv'\n",
    "    sourceDF,_ = aISDM.load_data_from_csv(sorceFile)\n",
    "    return sourceDF.loc[:,['LON','LAT']].to_numpy(), sourceDF.loc[0,'Length']\n",
    "\n",
    "get_traj_lon_lat_data_with_len(vesselDataSources[0].srcDir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of all such trajectories\n",
    "#this is training data\n",
    "trajSeqList = []\n",
    "typeList = []\n",
    "diffVessel = 0\n",
    "for vesselDataSource in vesselDataSources:\n",
    "    print(\"Taking data from:%s\"%vesselDataSource.srcDir)\n",
    "    for trajNum in range(0,vesselDataSource.trainNum):\n",
    "        trajSeqList.append(get_traj_lon_lat_data_with_len(vesselDataSource.srcDir,trajNum))\n",
    "        #append the type information as well\n",
    "        typeList.append(diffVessel)\n",
    "    diffVessel = diffVessel + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajSeqList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seq_to_x_y_lon_lat(seq, typeVessel, lenShip):\n",
    "    #first column\n",
    "    #-2 is is to take care of boundary condition\n",
    "    #since we are considering 2 time stamps for the input data\n",
    "    firstTwoCol = seq[:-2,:].copy()\n",
    "    secTwoCol = seq[1:-1,:].copy()\n",
    "\n",
    "    outputLabel = seq[2:,:].copy()\n",
    "    \n",
    "    xDataRow = firstTwoCol.shape[0]\n",
    "    typeData = np.zeros((xDataRow,2))\n",
    "    #either 0 or 1\n",
    "    typeData[:,typeVessel] = 1\n",
    "    \n",
    "    lenData = np.zeros((xDataRow,1))\n",
    "    lenData[:,:] = lenShip\n",
    "    #FIXME make this more generic\n",
    "    xData = np.hstack((firstTwoCol,typeData,lenData,secTwoCol,typeData,lenData))\n",
    "    return xData, outputLabel\n",
    "convert_seq_to_x_y_lon_lat(trajSeqList[0][0],typeList[0],trajSeqList[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate throgh trajSeqList \n",
    "#and keep on stacking them vertically\n",
    "#to make giant input and output matrix\n",
    "xData = np.zeros((0,10))\n",
    "yData = np.zeros((0,2))\n",
    "print(xData.shape)\n",
    "print(yData.shape)\n",
    "for trajNum in range(len(trajSeqList)):\n",
    "    if((trajSeqList[trajNum][0].shape[0]) > 2):\n",
    "        xTemp,yTemp = convert_seq_to_x_y_lon_lat(trajSeqList[trajNum][0],typeList[trajNum],trajSeqList[trajNum][1])\n",
    "        xData = np.vstack((xData,xTemp.copy()))\n",
    "        yData = np.vstack((yData,yTemp.copy()))\n",
    "        \n",
    "print(xData)\n",
    "print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xData.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToStore = dataDir + \"XData.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "np.save(xToStore, xData)\n",
    "np.save(yToStore, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToStore = dataDir + \"XData.npy\"\n",
    "yToStore = dataDir + \"YData.npy\"\n",
    "\n",
    "xData = np.load(xToStore)\n",
    "yData = np.load(yToStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xData.shape)\n",
    "print(yData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the min and max stats of len\n",
    "lenListNP = xData[:,-1]\n",
    "lenListNPMin = np.min(lenListNP)\n",
    "lenListNPMax = np.max(lenListNP)\n",
    "print(lenListNPMin)\n",
    "print(lenListNPMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the data\n",
    "xLonLatData_0 = (xData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "xLonLatData_0 = np.reshape(xLonLatData_0,(xLonLatData_0.shape[0],1))\n",
    "xLonLatData_1 = (xData[:,1] - latMin)/(latMax - latMin)\n",
    "xLonLatData_1 = np.reshape(xLonLatData_1,(xLonLatData_1.shape[0],1))\n",
    "\n",
    "xLonLatData_2 = xData[:,2].copy()\n",
    "xLonLatData_2 = np.reshape(xLonLatData_2,(xLonLatData_2.shape[0],1))\n",
    "xLonLatData_3 = xData[:,3].copy()\n",
    "xLonLatData_3 = np.reshape(xLonLatData_3,(xLonLatData_3.shape[0],1))\n",
    "\n",
    "xLonLatData_4 = (xData[:,4] - lenListNPMin)/(lenListNPMax - lenListNPMin)\n",
    "xLonLatData_4 = np.reshape(xLonLatData_4,(xLonLatData_4.shape[0],1))\n",
    "\n",
    "xLonLatData_5 = (xData[:,5] - lonMin)/(lonMax - lonMin)\n",
    "xLonLatData_5 = np.reshape(xLonLatData_5,(xLonLatData_5.shape[0],1))\n",
    "xLonLatData_6 = (xData[:,6] - latMin)/(latMax - latMin)\n",
    "xLonLatData_6 = np.reshape(xLonLatData_6,(xLonLatData_6.shape[0],1))\n",
    "\n",
    "xLonLatData_7 = xData[:,7].copy()\n",
    "xLonLatData_7 = np.reshape(xLonLatData_7,(xLonLatData_7.shape[0],1))\n",
    "xLonLatData_8 = xData[:,8].copy()\n",
    "xLonLatData_8 = np.reshape(xLonLatData_8,(xLonLatData_8.shape[0],1))\n",
    "\n",
    "xLonLatData_9 = (xData[:,9] - lenListNPMin)/(lenListNPMax - lenListNPMin)\n",
    "xLonLatData_9 = np.reshape(xLonLatData_9,(xLonLatData_9.shape[0],1))\n",
    "\n",
    "xLonLatDataNorm = np.hstack((xLonLatData_0,xLonLatData_1  \\\n",
    "                             ,xLonLatData_2,xLonLatData_3 \\\n",
    "                             ,xLonLatData_4,xLonLatData_5 \\\n",
    "                             ,xLonLatData_6,xLonLatData_7 \\\n",
    "                             ,xLonLatData_8,xLonLatData_9 \\\n",
    "                            ))\n",
    "\n",
    "xLonLatDataNorm = np.reshape(xLonLatDataNorm,(xLonLatDataNorm.shape[0], 2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise the output data as well\n",
    "yLonLatData_0 = (yData[:,0] - lonMin)/(lonMax - lonMin)\n",
    "yLonLatData_0 = np.reshape(yLonLatData_0,(yLonLatData_0.shape[0],1))\n",
    "yLonLatData_1 = (yData[:,1] - latMin)/(latMax - latMin)\n",
    "yLonLatData_1 = np.reshape(yLonLatData_1,(yLonLatData_1.shape[0],1))\n",
    "yLonLatDataNorm = np.hstack((yLonLatData_0,yLonLatData_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yLonLatDataNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xLonLatDataNorm[0,:])\n",
    "print(yLonLatDataNorm[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences= True, input_shape=(2,5)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(units=2, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xLonLatDataNorm, yLonLatDataNorm, epochs=500, batch_size = 512 , verbose = 2)\n",
    "# model.fit([xLonLatTSNorm, xTypeLenNorm], yLonLatDataNorm, epochs=200, batch_size = 512 , verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = dataDir + \"Model_500_MSE_2.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will take four arguments\n",
    "#[[lonPrev, latPrev, lonCurr, latCurr]] shape is 1x4 or nx4\n",
    "#can be vectorised also\n",
    "def normalize_lon_lat(arr):\n",
    "    #subtract the minimum \n",
    "    #and divide by range\n",
    "    ret0 = (arr[:,0] - lonMin)/(lonMax - lonMin)\n",
    "    ret0 = np.reshape(ret0, (ret0.shape[0],1))\n",
    "    ret1 = (arr[:,1] - latMin)/(latMax - latMin)\n",
    "    ret1 = np.reshape(ret1, (ret1.shape[0],1))\n",
    "    ret = np.hstack((ret0, ret1))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 time stamps of LON and LAT\n",
    "#normalises them and \n",
    "#and makes the prediction\n",
    "#de normalize the output\n",
    "#and return the values\n",
    "#prevTraj numpy array of 1x2\n",
    "#currTraj numpy array of 1x2\n",
    "def compute_30_min_pred(prevTraj, currTraj, typeVessel, lenShip):\n",
    "    \"\"\"\n",
    "    Compute prediction for 30 minutes.\n",
    "    \n",
    "    takes LON and LAT of three time stamps\n",
    "    normalises them\n",
    "    and makes prediction\n",
    "    and returns denormalised LON and LAT values\n",
    "    \"\"\"\n",
    "        \n",
    "    if(prevTraj.shape != (1,2)):\n",
    "        raise Exception('Shape of prevTraj should be 1x2')\n",
    "    \n",
    "    if(currTraj.shape != (1,2)):\n",
    "        raise Exception('Shape of currTraj should be 1x2')\n",
    "        \n",
    "    trajX = np.vstack((prevTraj,currTraj))\n",
    "    trajXNorm = normalize_lon_lat(trajX)\n",
    "    \n",
    "    \n",
    "    typeData = np.zeros((2,2))\n",
    "    #either 0 or 1\n",
    "    typeData[:,typeVessel] = 1\n",
    "    \n",
    "    lenData = np.zeros((2,1))\n",
    "    lenData[:,:] = lenShip\n",
    "    \n",
    "    lenData = (lenData - lenListNPMin)/(lenListNPMax - lenListNPMin)\n",
    "    \n",
    "    trajXNormData = np.hstack((trajXNorm,typeData,lenData))\n",
    "    \n",
    "    trajXNormData = np.reshape(trajXNormData,(1,2,5))\n",
    "    predLatLon = model.predict(trajXNormData)\n",
    "    \n",
    "    predLon = predLatLon[0,0]\n",
    "    predLat = predLatLon[0,1]\n",
    "    \n",
    "    #after prediction de normalise it\n",
    "    predLonScaled = (predLon * (lonMax - lonMin)) + lonMin\n",
    "    predLatScaled = (predLat * (latMax - latMin)) + latMin\n",
    "    return predLonScaled, predLatScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_30_min_pred(prevTraj, currTraj, typeVessel, lenShip, n = 1):\n",
    "    \"\"\"\n",
    "    Compute prediction for n*30 minutes.\n",
    "    \n",
    "    calls compute_30_min_pred\n",
    "    according to value of n\n",
    "    default value of n = 1\n",
    "    \"\"\"\n",
    "    #temp vaariables to store the previous trajectory \n",
    "    firstLoc = prevTraj.copy()\n",
    "    secLoc = currTraj.copy()\n",
    "    \n",
    "    #return value will be numpy array of nx2\n",
    "    ret = np.zeros((0,2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        #make prediction using 30 min pred\n",
    "        predLon, predLat = compute_30_min_pred(firstLoc, secLoc, typeVessel, lenShip)\n",
    "        #append them in ret\n",
    "        ret = np.vstack((ret,np.array([[predLon,predLat]])))\n",
    "        \n",
    "        #update firstLoc and secLoc \n",
    "        #for next  iteration\n",
    "        \n",
    "        firstLoc = secLoc.copy()\n",
    "        secLoc = np.reshape(ret[-1,:].copy(),(1,2))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_for_traj(srcDir, num, typeVessel):\n",
    "    \"\"\"\n",
    "    Compute error for the entire trajectory.\n",
    "    \n",
    "    Returns list of error for predictions of upto \n",
    "    4 hours i.e. 240 minutes.\n",
    "    \n",
    "    Parameters:\n",
    "        num (int): number of vessel trajectory \n",
    "                   whose prediction errors to be computed.\n",
    "    Returns:\n",
    "        errorVal (list of floats): list of errors in KM for each\n",
    "                                   30 min predictions.\n",
    "    \"\"\"    \n",
    "    errorVal = []\n",
    "    vesselTraj,lenData = get_traj_lon_lat_data_with_len(srcDir, num)\n",
    "\n",
    "    if(vesselTraj.shape[0] < 3):\n",
    "        return errorVal\n",
    "        \n",
    "        \n",
    "    predVesselTraj = compute_n_30_min_pred(np.reshape(vesselTraj[0,:], (1,2)), np.reshape(vesselTraj[1,:], (1,2)), typeVessel, lenData, n = 8)\n",
    "    predRange = vesselTraj.shape[0] - 2\n",
    "\n",
    "    #FIXME 8 can come from some variable too\n",
    "    #for 8 consecutive predictions\n",
    "    if(predRange > 8):\n",
    "        predRange = 8\n",
    "\n",
    "    #use trajectories which has atleast one\n",
    "    #value to predict\n",
    "    if(predRange >= 0):\n",
    "        for pred in range(predRange):    \n",
    "            trueLon, trueLat = vesselTraj[(2 + pred),0], vesselTraj[(2 + pred),1]\n",
    "            predLon, predLat = predVesselTraj[pred,0], predVesselTraj[pred,1]\n",
    "    #         print(trueLon, trueLat, '-', predLon, predLat)\n",
    "            errorVal.append(gC.compute_distance(trueLon, trueLat, predLon, predLat))\n",
    "    return errorVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(get_error_for_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1004 = []\n",
    "for traj in range(vesselDataSources[0].trainNum):\n",
    "    trainDataWholeErrors_1004.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainWholeErr_1004_30 = []\n",
    "trainWholeErr_1004_60 = []\n",
    "trainWholeErr_1004_90 = []\n",
    "trainWholeErr_1004_120 = []\n",
    "trainWholeErr_1004_150 = []\n",
    "trainWholeErr_1004_180 = []\n",
    "trainWholeErr_1004_210 = []\n",
    "trainWholeErr_1004_240 = []\n",
    "\n",
    "trainWholeErr_1004_n_30 = [trainWholeErr_1004_30 \\\n",
    ", trainWholeErr_1004_60 \\\n",
    ", trainWholeErr_1004_90 \\\n",
    ", trainWholeErr_1004_120 \\\n",
    ", trainWholeErr_1004_150 \\\n",
    ", trainWholeErr_1004_180 \\\n",
    ", trainWholeErr_1004_210 \\\n",
    ", trainWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    tempTick = (value*30) + 30\n",
    "    ret = \"%d\"%(tempTick)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErr_1004_30NP = np.array(trainWholeErr_1004_30)\n",
    "trainWholeErr_1004_60NP = np.array(trainWholeErr_1004_60)\n",
    "trainWholeErr_1004_90NP = np.array(trainWholeErr_1004_90)\n",
    "trainWholeErr_1004_120NP = np.array(trainWholeErr_1004_120)\n",
    "trainWholeErr_1004_150NP = np.array(trainWholeErr_1004_150)\n",
    "trainWholeErr_1004_180NP = np.array(trainWholeErr_1004_180)\n",
    "trainWholeErr_1004_210NP = np.array(trainWholeErr_1004_210)\n",
    "trainWholeErr_1004_240NP = np.array(trainWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWholeErrMean_1004 = [np.mean(trainWholeErr_1004_30NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_60NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_90NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_120NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_150NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_180NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_210NP) \\\n",
    "            ,np.mean(trainWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Training 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "testDataWholeErrors_1004 = []\n",
    "for traj in range(vesselDataSources[0].trainNum,vesselDataSources[0].testNumEnd):\n",
    "    testDataWholeErrors_1004.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testWholeErr_1004_30 = []\n",
    "testWholeErr_1004_60 = []\n",
    "testWholeErr_1004_90 = []\n",
    "testWholeErr_1004_120 = []\n",
    "testWholeErr_1004_150 = []\n",
    "testWholeErr_1004_180 = []\n",
    "testWholeErr_1004_210 = []\n",
    "testWholeErr_1004_240 = []\n",
    "\n",
    "testWholeErr_1004_n_30 = [testWholeErr_1004_30 \\\n",
    ", testWholeErr_1004_60 \\\n",
    ", testWholeErr_1004_90 \\\n",
    ", testWholeErr_1004_120 \\\n",
    ", testWholeErr_1004_150 \\\n",
    ", testWholeErr_1004_180 \\\n",
    ", testWholeErr_1004_210 \\\n",
    ", testWholeErr_1004_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testDataWholeErrors_1004:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testWholeErr_1004_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErr_1004_30NP = np.array(testWholeErr_1004_30)\n",
    "testWholeErr_1004_60NP = np.array(testWholeErr_1004_60)\n",
    "testWholeErr_1004_90NP = np.array(testWholeErr_1004_90)\n",
    "testWholeErr_1004_120NP = np.array(testWholeErr_1004_120)\n",
    "testWholeErr_1004_150NP = np.array(testWholeErr_1004_150)\n",
    "testWholeErr_1004_180NP = np.array(testWholeErr_1004_180)\n",
    "testWholeErr_1004_210NP = np.array(testWholeErr_1004_210)\n",
    "testWholeErr_1004_240NP = np.array(testWholeErr_1004_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWholeErrMean_1004 = [np.mean(testWholeErr_1004_30NP) \\\n",
    "            ,np.mean(testWholeErr_1004_60NP) \\\n",
    "            ,np.mean(testWholeErr_1004_90NP) \\\n",
    "            ,np.mean(testWholeErr_1004_120NP) \\\n",
    "            ,np.mean(testWholeErr_1004_150NP) \\\n",
    "            ,np.mean(testWholeErr_1004_180NP) \\\n",
    "            ,np.mean(testWholeErr_1004_210NP) \\\n",
    "            ,np.mean(testWholeErr_1004_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error Value On Testing 1004\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testWholeErrMean_1004,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "trainDataWholeErrors_1024 = []\n",
    "for traj in range(vesselDataSources[1].trainNum):\n",
    "    trainDataWholeErrors_1024.append(get_error_for_traj(vesselDataSources[1].srcDir,traj,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store errors for all training trajectories\n",
    "testDataWholeErrors_1024 = []\n",
    "for traj in range(vesselDataSources[1].trainNum,vesselDataSources[1].testNumEnd):\n",
    "    testDataWholeErrors_1024.append(get_error_for_traj(vesselDataSources[1].srcDir,traj,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rand1000TrajTrain = np.random.randint(0,vesselDataSources[0].trainNum,size = 1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainR1000Err = []\n",
    "for traj in rand1000TrajTrain:\n",
    "    trainR1000Err.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "trainR1000Err_30 = []\n",
    "trainR1000Err_60 = []\n",
    "trainR1000Err_90 = []\n",
    "trainR1000Err_120 = []\n",
    "trainR1000Err_150 = []\n",
    "trainR1000Err_180 = []\n",
    "trainR1000Err_210 = []\n",
    "trainR1000Err_240 = []\n",
    "\n",
    "trainR1000Err_n_30 = [trainR1000Err_30 \\\n",
    ", trainR1000Err_60 \\\n",
    ", trainR1000Err_90 \\\n",
    ", trainR1000Err_120 \\\n",
    ", trainR1000Err_150 \\\n",
    ", trainR1000Err_180 \\\n",
    ", trainR1000Err_210 \\\n",
    ", trainR1000Err_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in trainR1000Err:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            trainR1000Err_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainR1000Err_30NP = np.array(trainR1000Err_30)\n",
    "trainR1000Err_60NP = np.array(trainR1000Err_60)\n",
    "trainR1000Err_90NP = np.array(trainR1000Err_90)\n",
    "trainR1000Err_120NP = np.array(trainR1000Err_120)\n",
    "trainR1000Err_150NP = np.array(trainR1000Err_150)\n",
    "trainR1000Err_180NP = np.array(trainR1000Err_180)\n",
    "trainR1000Err_210NP = np.array(trainR1000Err_210)\n",
    "trainR1000Err_240NP = np.array(trainR1000Err_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainR1000ErrMean = [np.mean(trainR1000Err_30NP) \\\n",
    "            ,np.mean(trainR1000Err_60NP) \\\n",
    "            ,np.mean(trainR1000Err_90NP) \\\n",
    "            ,np.mean(trainR1000Err_120NP) \\\n",
    "            ,np.mean(trainR1000Err_150NP) \\\n",
    "            ,np.mean(trainR1000Err_180NP) \\\n",
    "            ,np.mean(trainR1000Err_210NP) \\\n",
    "            ,np.mean(trainR1000Err_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error for 1000 Training Trajectories\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(trainR1000ErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "rand1000TrajTest = np.random.randint(vesselDataSources[0].trainNum,vesselDataSources[0].testNumEnd,size = 1000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR1000Err = []\n",
    "for traj in rand1000TrajTrain:\n",
    "    testR1000Err.append(get_error_for_traj(vesselDataSources[0].srcDir,traj,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregate those list of errors into\n",
    "#list of 30 minErr, 60 minErr, ...\n",
    "testR1000Err_30 = []\n",
    "testR1000Err_60 = []\n",
    "testR1000Err_90 = []\n",
    "testR1000Err_120 = []\n",
    "testR1000Err_150 = []\n",
    "testR1000Err_180 = []\n",
    "testR1000Err_210 = []\n",
    "testR1000Err_240 = []\n",
    "\n",
    "testR1000Err_n_30 = [testR1000Err_30 \\\n",
    ", testR1000Err_60 \\\n",
    ", testR1000Err_90 \\\n",
    ", testR1000Err_120 \\\n",
    ", testR1000Err_150 \\\n",
    ", testR1000Err_180 \\\n",
    ", testR1000Err_210 \\\n",
    ", testR1000Err_240 \\\n",
    "]\n",
    "\n",
    "for trajErr in testR1000Err:\n",
    "    #take the list\n",
    "    for n_30 in range(8):\n",
    "        if(len(trajErr) > n_30):\n",
    "            testR1000Err_n_30[n_30].append(trajErr[n_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR1000Err_30NP = np.array(testR1000Err_30)\n",
    "testR1000Err_60NP = np.array(testR1000Err_60)\n",
    "testR1000Err_90NP = np.array(testR1000Err_90)\n",
    "testR1000Err_120NP = np.array(testR1000Err_120)\n",
    "testR1000Err_150NP = np.array(testR1000Err_150)\n",
    "testR1000Err_180NP = np.array(testR1000Err_180)\n",
    "testR1000Err_210NP = np.array(testR1000Err_210)\n",
    "testR1000Err_240NP = np.array(testR1000Err_240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR1000ErrMean = [np.mean(testR1000Err_30NP) \\\n",
    "            ,np.mean(testR1000Err_60NP) \\\n",
    "            ,np.mean(testR1000Err_90NP) \\\n",
    "            ,np.mean(testR1000Err_120NP) \\\n",
    "            ,np.mean(testR1000Err_150NP) \\\n",
    "            ,np.mean(testR1000Err_180NP) \\\n",
    "            ,np.mean(testR1000Err_210NP) \\\n",
    "            ,np.mean(testR1000Err_240NP) \\\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"Average Error for 1000 Testing Trajectories\")\n",
    "ax.set_ylabel(\"Distance in KM\")\n",
    "ax.set_xlabel(\"Time in Minutes\")\n",
    "ax.plot(testR1000ErrMean,label = \"LSTM\")\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"trainWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, trainWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testWholeErr_1004_30NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_30NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_60NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_60NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_90NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_90NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_120NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_120NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_150NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_150NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_180NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_180NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_210NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_210NP)\n",
    "dataToStore = dataDir + \"testWholeErr_1004_240NP.npy\"\n",
    "np.save(dataToStore, testWholeErr_1004_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"trainR1000Err_30NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_30NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_60NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_60NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_90NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_90NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_120NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_120NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_150NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_150NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_180NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_180NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_210NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_210NP)\n",
    "dataToStore = dataDir + \"trainR1000Err_240NP.npy\"\n",
    "np.save(dataToStore, trainR1000Err_240NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToStore = dataDir + \"testR1000Err_30NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_30NP)\n",
    "dataToStore = dataDir + \"testR1000Err_60NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_60NP)\n",
    "dataToStore = dataDir + \"testR1000Err_90NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_90NP)\n",
    "dataToStore = dataDir + \"testR1000Err_120NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_120NP)\n",
    "dataToStore = dataDir + \"testR1000Err_150NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_150NP)\n",
    "dataToStore = dataDir + \"testR1000Err_180NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_180NP)\n",
    "dataToStore = dataDir + \"testR1000Err_210NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_210NP)\n",
    "dataToStore = dataDir + \"testR1000Err_240NP.npy\"\n",
    "np.save(dataToStore, testR1000Err_240NP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
